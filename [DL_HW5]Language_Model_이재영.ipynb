{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appletreeleaf/Study_Log/blob/DL/%5BDL_HW5%5DLanguage_Model_%EC%9D%B4%EC%9E%AC%EC%98%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR26RFkwXtvi"
      },
      "source": [
        "# **[HW5] Language Model**\n",
        "1. DataLoader\n",
        "2. Model\n",
        "3. Trainer\n",
        "4. Generation\n",
        "\n",
        "이번 실습에서는 RNN기반의 Language Model를 구현해서 텍스트를 직접 생성해보는 실습을 진행해보겠습니다.\n",
        "\n",
        "- dataset: WikiText2 (https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2)\n",
        "- model: LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crVJ36mMlaXP"
      },
      "source": [
        "\n",
        "\n",
        "## Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpvlE_XOWS33"
      },
      "source": [
        "런타임의 유형을 변경해줍니다.\n",
        "\n",
        "상단 메뉴에서 [런타임]->[런타임유형변경]->[하드웨어가속기]->[GPU]\n",
        "\n",
        "변경 이후 아래의 cell을 실행 시켰을 때, torch.cuda.is_avialable()이 True가 나와야 합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqVdEuPQzMAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4a6a45-e07e-4378-e0fb-c450ceb19dea"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o3-HPdHLZma"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import tqdm\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1GnKJCB4T_Q"
      },
      "source": [
        "# 1. DataLoader\n",
        "\n",
        "이전의 실습들에서 사용한것과 마찬가지로, PyTorch style의 dataloader를 먼저 만들어 두겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcNl0aWbS0OA"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "저희가 이번 실습에서 사용할 데이터셋은 Wikipedia에 있는 영문 글들을 가져온 WikiTree dataset입니다.\n",
        "저희가 불러올 데이터는 가장 작은 WikiTree dataset에서 자주 사용되지 않는 단어나 영어가 아닌 단어들은 <unk>으로 이미 전처리가 되어있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKf8zNuISiC2"
      },
      "source": [
        "import urllib\n",
        "with urllib.request.urlopen('https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/02-intermediate/language_model/data/train.txt') as f:\n",
        "    data = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBLNOlRKSpOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f82e4d-e271-46aa-9a80-4a358c597ce0"
      },
      "source": [
        "print('num_sentence:',len(data)) #42068개의 문장이 들어있다\n",
        "data[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_sentence: 42068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\" plans that give advertisers discounts for maintaining or increasing ad spending have become permanent <unk> at the news <unk> and underscore the fierce competition between newsweek time warner inc. 's time magazine and <unk> b. <unk> 's u.s. news & world report \\n\""
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfLTv1EPbSwj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "4d6a3647-0625-4d53-80af-6fbfecaafd58"
      },
      "source": [
        "seq_length_list = []\n",
        "for line in data:\n",
        "    seq_length_list.append(len(line.split())) # data에 있는 문장들을 split으로 쪼개서 문장들의 길이를 리스트에 추가함.\n",
        "\n",
        "counts, bins = np.histogram(seq_length_list, bins=20)\n",
        "plt.hist(bins[:-1], bins, weights=counts)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SdattmOcRwC"
      },
      "source": [
        "데이터에 있는 문장 길이들의 histogram을 볼 때 대부분의 data의 문장 길이가 50에 미치지 못하기 때문에 \\\\\n",
        "model에 집어넣을 최대 문장 길이를 50으로 세팅해두도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7MuFqsKcd4U"
      },
      "source": [
        "max_seq_len = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyMpsyX8TwYy"
      },
      "source": [
        "### Build Dictionary\n",
        "\n",
        "먼저 text 데이터를 모델에 넣어주기 위해서는 text에 존재하는 단어들을 index로 변환해주어야 합니다.\n",
        "\n",
        "이를 위해서는 단어를 index로 변환해주는 word2idx dictionary와 다시 index를 단어로 변환해주는 idx2word dictionary를 만들어야 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZmyZhcpTvZz"
      },
      "source": [
        "def build_dictionary(data, max_seq_len): # 단어를 index로 변환\n",
        "    word2idx = {} # Empty dict / word to index\n",
        "    idx2word = {} # Empty dict / index to word\n",
        "    ## Build Dictionary\n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = 1\n",
        "    idx2word[0] = '<pad>'\n",
        "    idx2word[1] = '<unk>'\n",
        "    idx = 2 # index 0은 <pad>, 1은 <unk>이므로 2부터 시작\n",
        "    for line in data:\n",
        "        words = line.decode('utf-8').split() # 단어를 문자열 단위로 쪼갬\n",
        "        words = words[:max_seq_len]          # 문장길이를 50으로 제한\n",
        "        ### Build Dictionary to convert word to index and index to word\n",
        "        ### YOUR CODE HERE (~ 5 lines)\n",
        "        for word in words:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = idx # word to index\n",
        "                idx2word[idx] = word # index to word\n",
        "                idx += 1\n",
        "\n",
        "\n",
        "    return word2idx, idx2word\n",
        "\n",
        "word2idx, idx2word = build_dictionary(data, max_seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPfV0OTc4Xdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58081c0-622d-4def-e3b1-c69077775fe6"
      },
      "source": [
        "if len(word2idx) == len(idx2word) == 10000: #중복이 없이 단어들을 dict로 만듬 // 두 dict의 길이가 같다\n",
        "    print(\"Test Passed!\")\n",
        "else:\n",
        "    raise AssertionError"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me_m8njoXHrv"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "이제 앞서 만든 dictionary를 이용해서 text로된 데이터셋을 index들로 변환시키겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6fuARgzXEDU"
      },
      "source": [
        "def preprocess(data, word2idx, idx2word, max_seq_len):\n",
        "    tokens = []\n",
        "    for line in data:\n",
        "        words = line.decode('utf-8').split() #한 줄씩\n",
        "        words = words[:max_seq_len] # 50개 까지 끊어어\n",
        "        ### Convert dataset with tokens\n",
        "        ### For each line, append <pad> token to match the number of max_seq_len\n",
        "        ### YOUR CODE HERE (~ 4 lines)\n",
        "        words += ['<pad>']*(max_seq_len - len(words)) # 50단어에 못미치는 문장들은 i, love, you, <pad>, <pad>... 이런식으로 길이를 50으로 맞춰줌\n",
        "        for word in words:\n",
        "            token = word2idx[word] # word to vector\n",
        "            tokens.append(token) # encoding된 문장들을 모은 list\n",
        "\n",
        "    return tokens\n",
        "\n",
        "tokens = preprocess(data, word2idx, idx2word, max_seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhADm3QPzrv3",
        "outputId": "f75000cb-72f3-4601-c6f0-7eb55d767eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 26,\n",
              " 1,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 27,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 39,\n",
              " 1,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 1,\n",
              " 43,\n",
              " 32,\n",
              " 44,\n",
              " 45,\n",
              " 46,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 47,\n",
              " 1,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 48,\n",
              " 49,\n",
              " 41,\n",
              " 42,\n",
              " 50,\n",
              " 51,\n",
              " 52,\n",
              " 53,\n",
              " 54,\n",
              " 55,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 42,\n",
              " 56,\n",
              " 57,\n",
              " 58,\n",
              " 59,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 35,\n",
              " 60,\n",
              " 42,\n",
              " 61,\n",
              " 62,\n",
              " 63,\n",
              " 64,\n",
              " 65,\n",
              " 66,\n",
              " 67,\n",
              " 68,\n",
              " 69,\n",
              " 70,\n",
              " 35,\n",
              " 71,\n",
              " 72,\n",
              " 42,\n",
              " 73,\n",
              " 74,\n",
              " 75,\n",
              " 35,\n",
              " 46,\n",
              " 42,\n",
              " 76,\n",
              " 77,\n",
              " 64,\n",
              " 78,\n",
              " 79,\n",
              " 80,\n",
              " 27,\n",
              " 28,\n",
              " 81,\n",
              " 82,\n",
              " 83,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 32,\n",
              " 61,\n",
              " 84,\n",
              " 1,\n",
              " 40,\n",
              " 85,\n",
              " 1,\n",
              " 62,\n",
              " 78,\n",
              " 86,\n",
              " 32,\n",
              " 1,\n",
              " 87,\n",
              " 88,\n",
              " 89,\n",
              " 90,\n",
              " 64,\n",
              " 78,\n",
              " 91,\n",
              " 92,\n",
              " 93,\n",
              " 94,\n",
              " 95,\n",
              " 96,\n",
              " 97,\n",
              " 82,\n",
              " 98,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 99,\n",
              " 32,\n",
              " 100,\n",
              " 42,\n",
              " 101,\n",
              " 102,\n",
              " 1,\n",
              " 103,\n",
              " 93,\n",
              " 104,\n",
              " 66,\n",
              " 105,\n",
              " 106,\n",
              " 107,\n",
              " 1,\n",
              " 108,\n",
              " 109,\n",
              " 1,\n",
              " 67,\n",
              " 68,\n",
              " 108,\n",
              " 27,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 110,\n",
              " 111,\n",
              " 112,\n",
              " 113,\n",
              " 83,\n",
              " 79,\n",
              " 80,\n",
              " 35,\n",
              " 114,\n",
              " 81,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 117,\n",
              " 108,\n",
              " 118,\n",
              " 119,\n",
              " 101,\n",
              " 120,\n",
              " 121,\n",
              " 42,\n",
              " 122,\n",
              " 35,\n",
              " 123,\n",
              " 124,\n",
              " 64,\n",
              " 125,\n",
              " 101,\n",
              " 126,\n",
              " 64,\n",
              " 32,\n",
              " 127,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 35,\n",
              " 1,\n",
              " 1,\n",
              " 98,\n",
              " 56,\n",
              " 40,\n",
              " 128,\n",
              " 29,\n",
              " 129,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 130,\n",
              " 131,\n",
              " 132,\n",
              " 133,\n",
              " 28,\n",
              " 81,\n",
              " 134,\n",
              " 135,\n",
              " 136,\n",
              " 42,\n",
              " 61,\n",
              " 137,\n",
              " 138,\n",
              " 139,\n",
              " 140,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 141,\n",
              " 40,\n",
              " 142,\n",
              " 61,\n",
              " 108,\n",
              " 143,\n",
              " 144,\n",
              " 145,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 146,\n",
              " 1,\n",
              " 147,\n",
              " 32,\n",
              " 82,\n",
              " 148,\n",
              " 149,\n",
              " 32,\n",
              " 76,\n",
              " 113,\n",
              " 150,\n",
              " 42,\n",
              " 138,\n",
              " 151,\n",
              " 152,\n",
              " 153,\n",
              " 42,\n",
              " 32,\n",
              " 66,\n",
              " 105,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 130,\n",
              " 154,\n",
              " 142,\n",
              " 155,\n",
              " 156,\n",
              " 152,\n",
              " 157,\n",
              " 158,\n",
              " 159,\n",
              " 160,\n",
              " 161,\n",
              " 98,\n",
              " 162,\n",
              " 163,\n",
              " 1,\n",
              " 42,\n",
              " 164,\n",
              " 119,\n",
              " 1,\n",
              " 73,\n",
              " 165,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 166,\n",
              " 1,\n",
              " 167,\n",
              " 35,\n",
              " 168,\n",
              " 42,\n",
              " 82,\n",
              " 169,\n",
              " 32,\n",
              " 170,\n",
              " 73,\n",
              " 165,\n",
              " 48,\n",
              " 32,\n",
              " 171,\n",
              " 172,\n",
              " 42,\n",
              " 173,\n",
              " 174,\n",
              " 48,\n",
              " 164,\n",
              " 174,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 32,\n",
              " 1,\n",
              " 175,\n",
              " 98,\n",
              " 61,\n",
              " 54,\n",
              " 63,\n",
              " 108,\n",
              " 176,\n",
              " 177,\n",
              " 178,\n",
              " 108,\n",
              " 179,\n",
              " 180,\n",
              " 181,\n",
              " 32,\n",
              " 68,\n",
              " 108,\n",
              " 32,\n",
              " 182,\n",
              " 183,\n",
              " 48,\n",
              " 184,\n",
              " 87,\n",
              " 35,\n",
              " 185,\n",
              " 186,\n",
              " 42,\n",
              " 1,\n",
              " 108,\n",
              " 27,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 169,\n",
              " 27,\n",
              " 64,\n",
              " 27,\n",
              " 27,\n",
              " 187,\n",
              " 66,\n",
              " 105,\n",
              " 87,\n",
              " 32,\n",
              " 68,\n",
              " 113,\n",
              " 188,\n",
              " 32,\n",
              " 189,\n",
              " 98,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 75,\n",
              " 27,\n",
              " 190,\n",
              " 148,\n",
              " 191,\n",
              " 192,\n",
              " 87,\n",
              " 32,\n",
              " 193,\n",
              " 27,\n",
              " 154,\n",
              " 194,\n",
              " 79,\n",
              " 80,\n",
              " 195,\n",
              " 196,\n",
              " 32,\n",
              " 197,\n",
              " 198,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 199,\n",
              " 42,\n",
              " 32,\n",
              " 200,\n",
              " 201,\n",
              " 76,\n",
              " 154,\n",
              " 1,\n",
              " 202,\n",
              " 203,\n",
              " 195,\n",
              " 87,\n",
              " 204,\n",
              " 1,\n",
              " 73,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 32,\n",
              " 205,\n",
              " 42,\n",
              " 27,\n",
              " 74,\n",
              " 169,\n",
              " 206,\n",
              " 1,\n",
              " 207,\n",
              " 73,\n",
              " 48,\n",
              " 1,\n",
              " 54,\n",
              " 208,\n",
              " 209,\n",
              " 80,\n",
              " 197,\n",
              " 32,\n",
              " 82,\n",
              " 98,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 32,\n",
              " 1,\n",
              " 210,\n",
              " 40,\n",
              " 35,\n",
              " 211,\n",
              " 212,\n",
              " 75,\n",
              " 213,\n",
              " 42,\n",
              " 214,\n",
              " 148,\n",
              " 215,\n",
              " 1,\n",
              " 202,\n",
              " 98,\n",
              " 166,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjyvqMgbZnfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a839aed-311e-4d38-ed68-2fb8bea7afcc"
      },
      "source": [
        "if len(tokens) == 2103400: #42068*50 -> 2103400*1 문장들을 일렬로 쭉 늘어놓은 길이\n",
        "    print(\"Test Passed!\")\n",
        "else:\n",
        "    raise AssertionError"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmQxX3BH-SAv"
      },
      "source": [
        "이제 전처리된 Token들을 문장 단위의 배열로 변환시켜 두겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knMvtp23-Jye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf7b6dd-769d-4b20-ade8-1a36515c8b34"
      },
      "source": [
        "tokens = np.array(tokens).reshape(-1, max_seq_len) # 열의 수를 max_seq_len=50으로 하고 행의 수는 자동으로 맞춰라\n",
        "print(tokens.shape)\n",
        "tokens[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42068, 50)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([745,  93, 746, 739, 747, 181, 748, 467, 749, 740, 750, 154, 751,\n",
              "       752,   1, 160,  32, 753,   1,  48, 754,  32, 755, 756, 757, 728,\n",
              "       555, 758,  99, 119, 555, 733,  48,   1, 759,   1, 119, 237, 753,\n",
              "       230, 760, 347,   0,   0,   0,   0,   0,   0,   0,   0])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pceBqmtTZ9g9"
      },
      "source": [
        "### DataLoader\n",
        "\n",
        "이제 전처리된 dataset을 활용하여 PyTorch style의 dataset과 dataloader를 만들도록 하겠습니다.\n",
        "\n",
        "Token형태의 데이터를 PyTorch 스타일의 dataset으로 만들 때 주의할 점은, 추후 embedding matrix에서 indexing을 해주기 위해서 각 token이 LongTensor 형태로 정의되어야 한다는 점입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hAwhG1K9iBI"
      },
      "source": [
        "class LMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokens):\n",
        "        super(LMDataset, self).__init__()\n",
        "        self.PAD = 0\n",
        "        self.UNK = 1\n",
        "        self.tokens = tokens\n",
        "        self._getitem(2)\n",
        "\n",
        "    def _getitem(self, index):\n",
        "        X = self.tokens[index]\n",
        "        y = np.concatenate((X[1:], [self.PAD])) # 왜 1부터 시작?\n",
        "\n",
        "        X = torch.from_numpy(X).unsqueeze(0).long() # token to long shape\n",
        "        y = torch.from_numpy(y).unsqueeze(0).long()\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.tokens[index]\n",
        "        y = np.concatenate((X[1:], [self.PAD]))\n",
        "\n",
        "        X = torch.from_numpy(X).long()\n",
        "        y = torch.from_numpy(y).long()\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiLNqM6kAda1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25395fe3-6c43-4eab-a645-e9ce269f1c37"
      },
      "source": [
        "batch_size = 64 #전체 데이터를 64개씩 묶어서 학습하겠다다\n",
        "dataset = LMDataset(tokens) # PyTorch 스타일로 변경\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True) # shuffle은 1 epochs마다 데이터를 섞겠다.\n",
        "\n",
        "print(len(dataset))\n",
        "print(len(dataloader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42068\n",
            "658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1nhBnqWxw4a"
      },
      "source": [
        "# 2. Model\n",
        "\n",
        "이번 section에서는 Language Modeling을 위한 Recurrent Model을 직접 만들어보도록 하겠습니다.\n",
        "\n",
        "Standard한 Recurrent Neural Network (RNN) model은 vanishing gradient 문제에 취약하기 때문에, 이번 실습에서는 변형된 RNN구조인 LSTM model을 활용하도록 하겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOoNVt3MDOjl"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lycT_9vwaJN"
      },
      "source": [
        "LSTM model의 전체적인 구조와 각 gate의 수식은 아래와 같습니다.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1n93tpNW55Xl4GxZNcJcbUVRhuNCGH38h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1h6nfvYwN8n"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1nH9U5iD9cO6OVVTbrx-LjypRvcWzbOCU)\n",
        "\n",
        "LSTM의 자세한 동작방식이 궁금하신 분은 아래의 블로그를 참조해주세요.\n",
        "\n",
        "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDNAysVqxxOk"
      },
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        # input-gate : 새로운 정보의 반영비\n",
        "        self.Wi = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # forget-gate : 이전 메모리 중 불필요한 부분 제거\n",
        "        self.Wf = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # gate-gate : 현재 단계에서 새로운 정보 생성\n",
        "        self.Wg = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # output-gate : 이전 상태의 출력량\n",
        "        self.Wo = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "        # non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x, h_0, c_0):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            input (x): [batch_size, input_size]\n",
        "            hidden_state (h_0): [batch_size, hidden_size]\n",
        "            cell_state (c_0): [batch_size, hidden_size]\n",
        "        Outputs\n",
        "            next_hidden_state (h_1): [batch_size, hidden_size]\n",
        "            next_cell_state (c_1): [batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        h_1, c_1 = None, None\n",
        "        input = torch.cat((x, h_0), 1) # ((input_data, prev_hidden state), c_0)\n",
        "        # Implement LSTM cell as noted above\n",
        "        ### YOUR CODE HERE (~ 6 lines)\n",
        "        i = self.sigmoid(self.Wi(input))\n",
        "        f = self.sigmoid(self.Wf(input))\n",
        "        g = self.tanh(self.Wg(input))\n",
        "        o = self.sigmoid(self.Wo(input))\n",
        "        c_1 = f * c_0 + i * g\n",
        "        h_1 = o * self.tanh(c_1)\n",
        "\n",
        "        return h_1, c_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Tff2VCJ56D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d5310a0-ccb4-4427-8f73-a08527b3f425"
      },
      "source": [
        "def test_lstm():\n",
        "    batch_size = 2\n",
        "    input_size = 5\n",
        "    hidden_size = 3\n",
        "\n",
        "    #torch.manual_seed(1234)\n",
        "    lstm = LSTMCell(input_size ,hidden_size)\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.constant_(m.weight, 0.1)\n",
        "            m.bias.data.fill_(0.01)\n",
        "    lstm.apply(init_weights)\n",
        "\n",
        "    x = torch.ones(batch_size, input_size)    # input\n",
        "    hx = torch.zeros(batch_size, hidden_size) # hidden state\n",
        "    cx = torch.zeros(batch_size, hidden_size) # cell\n",
        "\n",
        "    hx, cx = lstm(x, hx, cx) # lstm을 적용시킨 후의 hx, cx\n",
        "    assert hx.detach().allclose(torch.tensor([[0.1784, 0.1784, 0.1784],\n",
        "                                              [0.1784, 0.1784, 0.1784]]), atol=2e-1), \\\n",
        "            f\"Output of the hidden state does not match.\"\n",
        "    assert cx.detach().allclose(torch.tensor([[0.2936, 0.2936, 0.2936],\n",
        "                                              [0.2936, 0.2936, 0.2936]]), atol=2e-1), \\\n",
        "            f\"Output of the cell state does not match.\"\n",
        "\n",
        "    print(\"==LSTM cell test passed!==\")\n",
        "\n",
        "test_lstm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==LSTM cell test passed!==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DxU-78B33dG"
      },
      "source": [
        "## Language Model\n",
        "\n",
        "이제, 위에서 정의한 LSTM Cell을 활용해서 아래와 같은 Langauge Model을 만들어보도록 하겠습니다.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1nMAbL-g31nERM44dgohA3k9Vj_92hIh-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0U2s0hux_n6"
      },
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, input_size=64, hidden_size=64, vocab_size=10000):\n",
        "        super(LanguageModel, self).__init__() #왜 선언하는지 모르겠다.\n",
        "\n",
        "        self.input_layer = nn.Embedding(vocab_size, input_size) # word -> inteager -> look-up table(여기서 word2ix ?)/ 해당정수를 인덱스로 갖는 벡터 반환 -> 인풋 벡터가 된다.\n",
        "        self.hidden_layer = LSTMCell(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, hx, cx, predict=False):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            input (x): [batch_size]\n",
        "            hidden_state (h_0): [batch_size, hidden_size]\n",
        "            cell_state (c_0): [batch_size, hidden_size]\n",
        "            predict: whether to predict and sample the next word\n",
        "        Outputs\n",
        "            output (ox): [batch_size, hidden_size]\n",
        "            next_hidden_state (h_1): [batch_size, hidden_size]\n",
        "            next_cell_state (c_1): [batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        x = self.input_layer(x)\n",
        "        hx, cx = self.hidden_layer(x, hx, cx)\n",
        "        ox = self.output_layer(hx)\n",
        "\n",
        "        if predict == True:\n",
        "            probs = F.softmax(ox, dim=1) # softmax a1\n",
        "            # torch distribution allows sampling operation\n",
        "            # see https://pytorch.org/docs/stable/distributions.html\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            ox = dist.sample()\n",
        "\n",
        "        return ox, hx, cx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-ZpuMhsbBS8"
      },
      "source": [
        "# 3. Trainer\n",
        "\n",
        "자 이제 위에서 구현한 dataloader와 langauge model을 활용해서 모델의 학습을 진행해보도록 하겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7TY7HmvbRlB"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self,\n",
        "                 word2idx,\n",
        "                 idx2word,\n",
        "                 dataloader,\n",
        "                 model,\n",
        "                 criterion,\n",
        "                 optimizer,\n",
        "                 device):\n",
        "        \"\"\"\n",
        "        dataloader: dataloader\n",
        "        model: langauge model\n",
        "        criterion: loss function to evaluate the model (e.g., BCE Loss)\n",
        "        optimizer: optimizer for model\n",
        "        \"\"\"\n",
        "        self.word2idx = word2idx\n",
        "        self.idx2word = idx2word\n",
        "        self.dataloader = dataloader\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "\n",
        "    def train(self, epochs = 1):\n",
        "        self.model.to(self.device) # transfer gpu\n",
        "        start_time = time.time()\n",
        "        for epoch in range(epochs):\n",
        "            losses = []\n",
        "            for iter, (x_batch, y_batch) in tqdm.tqdm(enumerate(self.dataloader)):\n",
        "                self.model.train()\n",
        "\n",
        "                batch_size, max_seq_len = x_batch.shape\n",
        "                x_batch = x_batch.to(self.device)\n",
        "                y_batch = y_batch.to(self.device)\n",
        "\n",
        "                # initial hidden-states\n",
        "                hx = torch.zeros(batch_size, hidden_size).to(self.device)\n",
        "                cx = torch.zeros(batch_size, hidden_size).to(self.device)\n",
        "\n",
        "                # Implement LSTM operation\n",
        "                ox_batch = []\n",
        "                # Get output logits for each time sequence and append to the list, ox_batch\n",
        "                # YOUR CODE HERE (~ 4 lines)\n",
        "                for s_idx in range(max_seq_len):\n",
        "                    x = x_batch[:, s_idx]\n",
        "                    ox, hx, cx = self.model(x, hx, cx)\n",
        "                    ox_batch.append(ox)\n",
        "                # outputs are ordered by the time sequence\n",
        "                ox_batch = torch.cat(ox_batch).reshape(max_seq_len, batch_size, -1)\n",
        "                ox_batch = ox_batch.permute(1,0,2).reshape(batch_size*max_seq_len, -1)\n",
        "                y_batch = y_batch.reshape(-1)\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                loss = self.criterion(ox_batch, y_batch)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            end_time = time.time() - start_time\n",
        "            end_time = str(datetime.timedelta(seconds=end_time))[:-7]\n",
        "            print('Time [%s], Epoch [%d/%d], loss: %.4f'\n",
        "                  % (end_time, epoch+1, epochs, np.mean(losses)))\n",
        "            if epoch % 5 == 0:\n",
        "                generated_sentences = self.test()\n",
        "                print('[Generated Sentences]')\n",
        "                for sentence in generated_sentences:\n",
        "                    print(sentence)\n",
        "\n",
        "    def test(self):\n",
        "        # Test model to genereate the sentences\n",
        "        self.model.eval()\n",
        "        num_sentence = 5\n",
        "        max_seq_len = 50\n",
        "\n",
        "        # initial hidden-states\n",
        "        outs = []\n",
        "        x = torch.randint(0, 10000, (num_sentence,)).to(self.device)\n",
        "        hx = torch.zeros(num_sentence, hidden_size).to(self.device)\n",
        "        cx = torch.zeros(num_sentence, hidden_size).to(self.device)\n",
        "\n",
        "        outs.append(x)\n",
        "        with torch.no_grad():\n",
        "            for s_idx in range(max_seq_len-1):\n",
        "                x, hx, cx = self.model(x, hx, cx, predict=True)\n",
        "                outs.append(x)\n",
        "        outs = torch.cat(outs).reshape(max_seq_len, num_sentence)\n",
        "        outs = outs.permute(1, 0)\n",
        "        outs = outs.detach().cpu().numpy()\n",
        "\n",
        "        sentences = []\n",
        "        for out in outs:\n",
        "            sentence = []\n",
        "            for token_idx in out:\n",
        "                word = self.idx2word[token_idx]\n",
        "                sentence.append(word)\n",
        "            sentences.append(sentence)\n",
        "\n",
        "        return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1EE9KDvyeeF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "129eb35e-3bac-4eae-a543-b520e1e2756a"
      },
      "source": [
        "for iter,(x_batch,y_batch) in (enumerate(dataloader)):\n",
        "    print(x_batch.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([84, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgEJv1vWqNkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5731c572-7556-4da4-84c6-e4e2e4fa80fe"
      },
      "source": [
        "lr = 1e-2\n",
        "input_size = 128\n",
        "hidden_size = 128\n",
        "batch_size = 256\n",
        "\n",
        "dataset = LMDataset(tokens)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "model = LanguageModel(input_size=input_size, hidden_size=hidden_size)\n",
        "# NOTE: you should use ignore_index to ignore the loss from predicting the <PAD> token\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "device = torch.device('cuda')\n",
        "\n",
        "trainer = Trainer(word2idx = word2idx,\n",
        "                  idx2word = idx2word,\n",
        "                  dataloader=dataloader,\n",
        "                  model = model,\n",
        "                  criterion=criterion,\n",
        "                  optimizer = optimizer,\n",
        "                  device=device)\n",
        "\n",
        "trainer.train(epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:00:15], Epoch [1/50], loss: 6.0491\n",
            "[Generated Sentences]\n",
            "['underscore', 'drinking', 'than', 'yesterday', 'said', 'it', 'was', 'up', 'residents', 'and', 'on', 'britain', 'said', 'that', 'the', 'agreement', '<unk>', 'treatment', 'when', 'uncertainty', 'for', 'precisely', '$', 'N', 'on', 'N', 'billion', 'nyse', 'began', 'a', '$', 'N', 'million', 'charge', 'of', 'the', 'last', 'raising', 'great', 'include', 'broader', 'agreement', 'and', 'frustrated', 'than', 'N', 'N', 'more', 'issued', 'in']\n",
            "['foam', 'leave', 'dollars', 'in', 'a', 'board', 'area', 'and', 'el', 'institutional', 'high', 'interest', 'figures', 'are', 'dragged', 'right', 'small', 'money', 'of', 'these', 'states', 'of', 'looking', 'than', 'mr.', 'antar', 'also', 'expects', 'a', 'loss', 'from', 'steven', '<unk>', 'parental', 'prices', 'or', 'summoned', '<unk>', '<unk>', 'to', 'consider', 'how', 'pulls', 'just', 'an', 'boston', '<unk>', 'plants', 'to', 'withdraw']\n",
            "['palo', 'nights', 'have', \"n't\", 'been', 'hanging', 'directly', 'at', 'return', 'for', 'the', 'notion', 'to', 'accommodate', 'the', 'enemies', 'players', 'to', 'earn', 'that', 'cboe', 'australian', 'called', 'a', 'N', 'N', 'of', 'the', 'administration', 'is', 'more', 'than', 'its', 'N', 'N', 'figure', 'shearson', 'N', 'higher', 'in', 'N', 'have', '<unk>', 'across', 'its', 'shipyard', 'are', 'the', 'galileo', '<unk>']\n",
            "['omnibus', 'did', 'in', '<unk>', 'certain', 'shut', 'care', 'said', 'par', 'dollars', 'from', 'mr.', 'lorenzo', 'said', 'they', 'get', 'the', 'good', 'subsidiary', 'school', 'to', 'get', 'us', 'over', 'the', 'place', 'of', 'our', 'next', '<unk>', 'for', '<unk>', 'a', 'N', 'N', 'blank', 'spokeswoman', 'was', \"n't\", 'disclosed', 'to', 'seize', 'a', 'private', '<unk>', 'fare', 'of', 'project', 'that', 'program']\n",
            "['lack', 'operating', 'funding', 'a', 'month', 'or', 'sony', 'expects', 'to', '#', 'N', 'billion', 'face', 'of', 'arthur', 'offers', 'planned', 'as', 'low', 'and', 'analysts', 'say', 'mrs.', 'brothers', 'serves', 'has', 'continued', 'to', 'determine', 'two', 'weeks', 'ago', 'in', 'its', 'mr.', 'gonzalez', 'said', 'its', 'certificates', 'to', 'N', 'investigations', 'in', 'alleviate', 'by', 'flat', 'to', 'its', 'san', 'diego']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:00:31], Epoch [2/50], loss: 5.2110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:00:47], Epoch [3/50], loss: 4.8978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:01:03], Epoch [4/50], loss: 4.6893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:01:19], Epoch [5/50], loss: 4.5314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:01:35], Epoch [6/50], loss: 4.4050\n",
            "[Generated Sentences]\n",
            "['bets', 'parity', 'publicity', 'problems', 'at', 'eastern', \"'s\", 'new', 'declined', 'to', 'hold', 'a', 'record', 'for', '$', 'N', 'to', '$', 'N', 'million', 'from', '$', 'N', 'million', 'to', '$', 'N', 'million', 'or', 'N', 'cents', 'to', '$', 'N', 'million', 'or', 'so', 'change', 'the', 'difference', 'between', 'third-quarter', 'earnings', 'hurt', '$', 'N', 'billion', 'in', 'third-quarter', 'level']\n",
            "['unexpectedly', '<unk>', 'the', 'ems', 'might', 'not', 'it', 'is', 'heavily', 'with', 'india', 'highway', 'offices', 'in', 'N', 'and', '<unk>', 'in', '<unk>', 'to', 'kill', 'other', 'areas', 'such', 'as', 'independent', 'directors', 'further', 'promoting', '<unk>', 'intensive', 'exports', 'increased', 'in', 'the', 'week', 'after', 'buying', 'N', 'N', 'of', 'term', 'to', 'early', 'selling', 'prices', 'of', 'attention', 'to', '$']\n",
            "['rule', 'batibot', 'should', 'find', 'american', '<unk>', 'information', 'for', 'not', '<unk>', 'or', 'predictions', 'out', 'whichever', 'bugs', 'demonstrated', 'favorable', 'until', 'the', 'year-ago', 'period', 'was', 'raised', 'to', 'an', 'average', 'but', 'of', 'its', 'own', 'account', 'and', 'commerce', 'department', 'association', 'holding', 'gas', 'pipeline', '<unk>', 'and', 'salomon', 'brothers', 'inc', '<unk>', 'which', 'has', 'increased', 'them', 'to', 'meet']\n",
            "['inviting', 'electronic', 'fund', 'in', 'chicago', 'via', 'salomon', 'brothers', 'inc.', 'a', 'district', '<unk>', 'for', 'october', 'N', 'friday', 'after', 'sept.', 'N', 'when', 'nbc', 'buyers', 'for', 'tax', 'hitachi', 'affairs', 'inc.', 'columbus', 'federal', 'savings', 'and', 'N', 'million', 'shares', 'outstanding', 'of', 'international', 'source', '&', 'quist', 'in', 'N', 'to', 'general', 'motors', 'acceptance', 'corp.', 'the', 'company', \"'s\"]\n",
            "['disclose', 'prosecution', 'program', 'traders', 'warn', 'mr.', 'greenspan', 'that', 'would', 'provide', 'anyone', 'closely', 'linked', 'the', 'boys', \"'\", 'documents', 'said', 'benefit', 'activities', 'he', 'said', 'he', 'will', 'have', 'or', 'reduce', 'sir', 'alan', 'dispute', 'to', 'oust', 'murder', 'to', 'have', 'a', 'few', 'years', 'ago', 'with', 'business', 'information', 'and', 'coverage', 'options', 'and', 'pushed', 'polyethylene', 'a', 'new']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:18,  9.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:01:53], Epoch [7/50], loss: 4.2975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:02:09], Epoch [8/50], loss: 4.2074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:02:25], Epoch [9/50], loss: 4.1263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:02:41], Epoch [10/50], loss: 4.0564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:02:56], Epoch [11/50], loss: 3.9931\n",
            "[Generated Sentences]\n",
            "['cases', 'always', 'saying', 'that', 'price', 'declines', 'from', 'its', 'session', 'in', 'the', 'august', 'on', 'six-month', 'cds', 'do', 'are', 'falling', 'because', 'of', 'revenue', 'would', 'be', 'received', 'by', 'the', 'alternative', 'rep.', 'john', 'pope', 'slipped', 'to', '$', 'N', 'million', 'or', '$', 'N', 'a', 'share', 'or', '$', 'N', 'a', 'share', 'or', 'N', 'cents', 'a', 'share']\n",
            "['division', 'major', 'prices', 'fell', 'slightly', 'to', 'the', 'N', 'N', 'jump', 'in', 'a', 'pretax', 'charge', 'of', '$', 'N', 'million', 'and', 'honeywell', 'to', 'buy', 'a', '$', 'N', 'billion', 'amount', 'from', 'a', 'year', 'ago', 'but', 'worries', 'that', 'the', 'most', 'dramatic', 'situation', 'would', 'take', 'several', 'quarters', 'of', '<unk>', 'will', 'register', 'line', 'with', 'profits', 'in']\n",
            "['spoken', 'implicit', 'he', 'says', 'the', '<unk>', 'dam', 'would', 'completely', 'disrupt', 'access', 'to', 'this', 'week', 'of', '<unk>', '<unk>', 'rather', 'than', 'those', 'with', 'natural', 'resources', 'these', 'issues', 'were', \"n't\", 'very', 'frustrated', 'by', 'the', 'most', 'state', 'markets', 'and', 'confronted', 'with', 'shows', 'a', 'hotel-casino', 'on', 'a', 'relatively', 'higher', 'airline', 'industry', 'for', 'his', 'trial', 'is']\n",
            "['posts', 'which', 'are', 'can', 'in', 'one', 'place', 'in', 'dozens', 'of', 'such', '<unk>', 'characters', 'sports', 'for', 'it', 'supplied', 'by', 'citibank', 'publicity', 'bad', '<unk>', 'for', 'customers', 'almost', 'N', 'N', 'of', 'insurance', 'control', 'by', 'the', 'national', 'assembly', 'does', \"n't\", 'stand', 'for', 'anyone', 'planned', 'to', 'get', 'their', 'conversation', 'with', 'it', 'the', 'fruit', 'seeks', 'program']\n",
            "['forge', 'falcon', 'inc', 'evans', 'was', 'offered', 'to', 'new', 'york', 'life', 'insurance', 'canada', 'as', 'industries', 'because', 'of', 'dropping', 'net', 'income', 'of', '$', 'N', 'million', 'or', '$', 'N', 'a', 'share', 'a', 'year', 'earlier', 'in', 'the', 'second', 'quarter', 'compared', 'with', 'to', '$', 'N', 'million', 'or', 'N', 'cents', 'a', 'share', 'from', '$', 'N', 'million']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:03:13], Epoch [12/50], loss: 3.9379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:03:29], Epoch [13/50], loss: 3.8856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:03:45], Epoch [14/50], loss: 3.8401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:04:01], Epoch [15/50], loss: 3.7965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:04:17], Epoch [16/50], loss: 3.7590\n",
            "[Generated Sentences]\n",
            "['record', 'dates', 'shot', 'of', 'mistakenly', 'partly', 'offset', 'by', 'that', '<unk>', 'weekend', 'that', 'are', 'giving', 'warner', 'moves', 'if', 'economic', 'outlook', 'is', 'limited', 'and', 'i', 'ca', \"n't\", 'do', 'so', 'today', 'a', 'way', 'that', 'new', 'york', \"'s\", 'backing', 'there', 'are', 'vice', 'president', 'and', 'his', 'institute', 'for', 'president', '<unk>', 'and', 'symptoms', 'in', 'areas', 'charging']\n",
            "['equivalent', 'archrival', 'trans', 'nations', 'seek', 'a', 'lawsuit', 'represented', 'by', 'N', 'N', 'of', 'the', 'partnership', \"'s\", 'first', 'boston', 'service', 'and', '<unk>', 'countries', 'in', 'january', 'N', 'for', 'one', 'share', 'of', 'bethlehem', 'and', 'to', 'remain', 'independent', 'and', 'misleading', 'statements', 'over', 'the', 'foreign', 'purchasing', 'managers', 'index', 'and', 'buyers', 'shares', 'by', 'buying', 'foreign', 'ownership', 'banks']\n",
            "['parade', 'addition', 'to', 'the', 'effects', 'of', 'money', 'laundering', 'game', 'could', 'weaken', 'banks', 'in', 'recent', 'sessions', 'at', 'speeds', 'mr.', '<unk>', 'said', 'that', 'mr.', 'gorbachev', 'long', 'distance', 'itself', 'away', 'or', 'more', 'than', 'you', 'got', 'to', '<unk>', 'through', 'rare', 'enterprise', 'zone', '<unk>', 'deciding', 'on', 'routes', 'that', 'commercial', 'for', '<unk>', 'product', 'lines', 'in', 'order']\n",
            "['throwing', 'failures', 'paid', 'off', 'the', 'discussions', 'under', 'the', 'oct.', 'N', 'plunge', 'next', 'year', 'soon', 'after', 'advertising', 'at', 'some', 'point', 'where', 'the', 'deficits', 'will', 'lose', 'their', 'only', 'significant', 'picture', 'because', 'of', 'sony', \"'s\", 'flight', 'attendants', 'researchers', 'estimated', 'the', 'close', 'for', 'the', 'corner', 'on', 'the', 'list', 'business', 'by', 'americans', 'with', '<unk>', 'distributors']\n",
            "['unknown', 'section', 'N', 'mr.', '<unk>', 'says', 'that', 'state', 'agencies', 'became', 'known', 'investment', 'stance', 'aimed', 'properties', 'into', 'contra', 'rebels', 'this', 'fall', 'with', 'communism', '<unk>', 'and', 'was', 'launched', 'the', 'courts', 'for', 'those', 'who', 'are', 'lined', 'higher', 'than', 'anyone', 'money', 'manager', 'says', 'that', 'the', 'japanese', 'used', 'the', '<unk>', 'reason', 'development', 'where', 'is', 'important']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:04:33], Epoch [17/50], loss: 3.7225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:04:49], Epoch [18/50], loss: 3.6923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:05:05], Epoch [19/50], loss: 3.6636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:05:21], Epoch [20/50], loss: 3.6371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:05:36], Epoch [21/50], loss: 3.6098\n",
            "[Generated Sentences]\n",
            "['toshiba', 'corp.', 'has', \"n't\", 'presented', 'a', 'new', 'structure', 'to', 'be', 'more', '<unk>', 'and', 'fiercely', 'attention', 'to', 'the', 'nation', \"'s\", 'merchandise', 'that', 'will', 'come', 'down', 'on', 'the', '<unk>', '<unk>', 'of', 'its', '<unk>', 'that', 'he', 'previously', 'ordered', 'whether', 'you', 'are', 'signs', 'by', 'portraying', 'itself', 'to', '<unk>', 'the', '<unk>', 'of', 'the', 'voice', 'of']\n",
            "['prosecutor', 'at', 'this', 'level', 'does', 'they', 'have', 'to', 'come', 'up', 'to', 'a', 'picture', 'to', 'magnetic', 'stimulators', 'in', 'the', 'elections', 'over', 'his', 'shoulder', 'from', '<unk>', 'this', 'industry', 'organization', 'and', 'northeast', 'bancorp', 'with', 'assurance', 'displays', 'from', 'singapore', 'said', 'it', 'would', 'pay', 'claims', 'from', 'sunnyvale', 'to', 'be', 'fully', 'operational', 'by', 'an', '<unk>', 'with']\n",
            "['downgraded', 'by', 'sen.', 'mitchell', 'a', 'securities', 'as', 'a', 'consultant', 'to', '<unk>', 'americans', 'he', 'does', \"n't\", 'think', 'can', 'change', 'his', 'attention', 'to', 'attract', 'spring', \"'s\", 'progress', 'toward', 'a', 'number', 'of', 'investors', 'in', 'september', 'while', 'falling', 'claims', 'are', \"n't\", 'traded', 'below', 'year-earlier', 'levels', 'were', 'flat', 'at', 'N', 'cents', 'a', 'bushel', 'to', 'fall']\n",
            "['materials', 'successfully', 'challenged', 'its', 'machines', 'in', 'a', 'statement', 'from', '<unk>', 'in', 'july', 'N', 'and', 'N', 'have', 'crossed', 'the', 'proposal', 'to', 'gain', 'a', 'reserve', 'in', 'N', 'or', 'thursday', 'and', '<unk>', 'according', 'to', 'continued', 'ownership', 'and', 'continue', 'to', 'sell', 'the', 'fdic', 'it', 'will', 'do', 'most', 'of', 'their', 'own', 'projects', 'program', 'their', 'failed']\n",
            "['land', 'made', 'by', 'ford', 'and', 'other', 'shareholders', 'to', 'have', 'a', '<unk>', 'agreement', 'with', 'ab', 'skf', 'of', 'hearings', 'based', 'in', 'net', 'income', 'compared', 'with', 'N', 'cents', 'a', 'share', 'a', 'year', 'earlier', 'after', 'payments', 'to', 'size', 'for', 'insurance', 'and', 'its', 'operations', 'since', 'it', 'had', 'no', 'comment', 'to', 'rise', 'to', '$', 'N', 'million']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:05:52], Epoch [22/50], loss: 3.5874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:06:08], Epoch [23/50], loss: 3.5647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:06:24], Epoch [24/50], loss: 3.5432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:06:40], Epoch [25/50], loss: 3.5265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:06:56], Epoch [26/50], loss: 3.5114\n",
            "[Generated Sentences]\n",
            "['guard', 'feared', 'who', 'had', 'made', 'the', 'right', 'to', '<unk>', 'its', 'power', 'to', 'protect', 'alcohol', 'subsidies', 'for', 'a', 'single', 'exception', 'earlier', 'this', 'year', 'as', 'the', 'labor', '<unk>', 'of', 'the', 'economy', 'and', '<unk>', 'of', 'the', 'security', 'of', '<unk>', 'management', 'and', 'walter', 'law', 'mr.', 'lang', 'pumped', '$', 'N', 'right', 'to', 'pay', 'the', 'power']\n",
            "['spending', 'average', 'weekend', 'another', 'thrift', 'rights', 'spend', '$', 'N', 'million', 'of', 'tax', 'debt', 'of', '$', 'N', 'million', 'per', 'share', 'of', 'N', 'N', 'N', 'secured', 'liabilities', 'tuesday', 'while', 'N', 'N', 'and', 'N', 'N', 'stakes', 'in', '<unk>', 'laser', 'disks', 'and', 'chemical', 'industries', 'inc.', 'citing', 'cost-cutting', 'auto', 'maker', 'and', '<unk>', 'corp.', 'and', 'to']\n",
            "['strengthening', 'examine', 'horrible', 'holdings', 'using', 'environmental', 'litigation', 'aim', 'of', '<unk>', 'the', 'wall', 'street', 'journal', \"'s\", '<unk>', 'hospital', 'supply', 'says', 'his', 'death', 'was', 'rudolph', 'vice', 'president', 'in', 'charge', 'of', 'the', 'parent', 'to', 'asia', 'among', 'terms', 'of', 'a', 'general', 'electric', 'agreement', 'and', 'broken', 'say', 'investors', 'might', 'be', 'expected', 'to', 'begin', 'buying', 'N']\n",
            "['complaints', 'scholar', 'guy', 'admitted', 'with', 'disney', 'burned', 'people', 'here', 'needed', 'leg', 'a', 'window', 'on', 'the', 'truce', 'wish', 'authorities', 'headed', 'to', 'politicians', 'taking', 'caught', 'among', 'other', 'defendants', \"'\", 'travel', 'policies', 'to', 'monetary', 'unity', 'and', 'what', '<unk>', 'premiums', 'i', 'cross', '<unk>', 'seeing', 'many', 'government', 'the', 'attack', 'bridge', 'one', 'of', 'those', 'our', 'efforts']\n",
            "['apartments', 'commissioned', 'they', 'line', 'they', 'are', 'a', '<unk>', 'and', 'not', 'its', 'wish', 'to', 'survive', '<unk>', 'or', '<unk>', '<unk>', '<unk>', 'ever', 'and', 'president', 'carter', 'findings', 'would', 'be', 'postponed', 'until', 'growth', 'or', 'pay', 'their', 'inventories', 'from', 'the', 'rest', 'of', 'the', 'blood', 'increasing', 'a', 'public', '<unk>', 'courtroom', 'crop', 'brown', 'boveri', 'companies', 'also', 'were']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:07:11], Epoch [27/50], loss: 3.4940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:07:28], Epoch [28/50], loss: 3.4779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:07:44], Epoch [29/50], loss: 3.4647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:07:59], Epoch [30/50], loss: 3.4499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:08:15], Epoch [31/50], loss: 3.4388\n",
            "[Generated Sentences]\n",
            "['fails', 'financially', 'it', 'is', 'involved', 'in', 'the', 'agreement', 'showing', 'the', 'white', 'house', 'on', 'federal', 'banks', 'that', 'will', 'restrict', 'the', 'extra', 'doctor', 'of', 'finding', '<unk>', 'at', 'productivity', \"'s\", 'declines', 'in', 'his', 'european', 'office', 'supply', 'before', 'they', 'buy', 'apples', 'and', 'failed', 'to', 'stay', '<unk>', 'where', 'she', 'was', 'sparked', 'by', 'an', '<unk>', '<unk>']\n",
            "['shouted', 'assumes', 'the', 'key', 'department', 'is', 'insured', 'because', 'of', 'weakness', 'in', 'the', 'coffee', 'business', 'this', 'fiscal', 'year', 'by', 'the', 'end', 'of', 'this', 'month', \"'s\", 'N', 'million', 'soviet', 'number', 'for', 'new', 'interest-rate', 'equivalent', 'of', '<unk>', 'strategies', '<unk>', 'sport', 'utility', 'hands', 'american', 'companies', 'in', 'the', 'west', 'of', 'international', 'areas', 'membership', 'in', 'joining']\n",
            "['dividends', 'journalists', 'to', 'do', 'a', 'higher', 'price', 'wars', 'at', 'N', 'off', 'the', 'day', 'at', 'citicorp', \"'s\", 'new', 'mgm', 'division', 'is', 'fighting', 'the', 'work', 'on', 'which', 'they', \"'d\", 'arrested', 'down', 'here', 'she', '<unk>', 'the', 'entire', 'helmsley', 'organization', 'dedicated', 'to', 'start', 'insisting', 'on', 'the', 'ocean', 'history', 'and', 'mr.', 'bush', \"'s\", 'dance', 'they']\n",
            "[\"o'kicki\", 'has', 'had', 'a', 'five-year', '<unk>', 'out', 'of', 'data', '<unk>', 'with', '<unk>', 'and', 'in', 'the', 'new', 'pipeline', 'and', 'the', 'market', '<unk>', 'mrs.', 'thatcher', \"'s\", 'first', '<unk>', 'indictment', 'is', 'a', 'lot', 'a', '<unk>', 'for', 'a', 'tour', 'break', 'through', 'the', 'movie', 'policy', 'when', 'foreign', 'policy', 'is', 'responsible', 'for', 'the', 'increasing', 'industry', 'has']\n",
            "['phelps', 'advocate', 'has', 'a', 'response', 'on', 'annual', 'revenue', 'and', 'pretax', 'profit', 'of', '$', 'N', 'million', 'or', 'five', 'cents', 'a', 'share', 'on', 'sales', 'of', '$', 'N', 'million', 'compared', 'with', 'a', 'fully', 'diluted', 'basis', 'as', 'of', 'these', 'are', 'pouring', '$', 'N', 'million', 'of', 'the', 'real', 'estate', 'norfolk', 'to', 'N', 'months', 'of', '$']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:08:31], Epoch [32/50], loss: 3.4275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:08:47], Epoch [33/50], loss: 3.4188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:09:03], Epoch [34/50], loss: 3.4072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:09:18], Epoch [35/50], loss: 3.3980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:09:34], Epoch [36/50], loss: 3.3888\n",
            "[Generated Sentences]\n",
            "['roebuck', 'technologies', 'inc.', 'said', 'the', 'suitor', 'schools', 'are', 'to', 'be', '<unk>', 'before', 'a', 'half-hour', 'rating', 'in', 'the', 'u.s.', 'with', 'cold', 'fusion', 'experiments', 'including', 'a', 'reluctant', 'witness', 'for', 'a', 'circuit', 'breaker', 'process', 'for', 'certain', 'drug-related', '<unk>', 'and', 'filled', 'with', 'the', '<unk>', 'institute', 'of', 'america', 'police', 'reported', 'that', 'investors', 'will', 'a', 'growing']\n",
            "['p&g', 'that', 'accounts', 'take', 'charge', 'of', 'our', 'own', 'over', 'the', 'world', 'bank', 'and', 'aerospace', 'concern', 'succeeding', 'chairman', 'breeden', 'said', 'his', 'son', 'would', 'head', 'the', 'economic', '<unk>', 'and', 'in', 'obtaining', 'one', 'hand', 'it', 'says', 'standard', '&', 'poor', \"'s\", 'corp.', 'all', 'join', '<unk>', 'pilots', 'will', 'receive', '$', 'N', 'but', 'were', \"n't\", 'clear']\n",
            "['abramson', 'mandate', 'from', 'deposits', 'and', 'closing', 'sales', 'also', 'held', 'up', 'N', 'to', 'N', 'million', 'francs', 'from', 'dec.', 'N', 'and', '$', 'N', 'million', 'respectively', 'more', 'than', '$', 'N', 'million', 'of', 'direct', 'tax', 'sales', 'and', 'expenses', 'including', 'senior', 'oppenheimer', 'and', 'chief', 'operating', 'officer', 'of', 'the', 'japanese', 'and', 'affiliate', 'normal', 'victims', 'of', 'florida']\n",
            "['scowcroft', 'p.m.', 'painewebber', 'inc.', 'chemical', 'bank', 'wcrs', 'said', 'third-quarter', 'profit', 'is', 'to', 'climb', 'on', 'mr.', 'maynard', \"'s\", 'acquisition', 'of', 'jaguar', 'plc', 'of', 'denver', 'and', 'the', 'three', 'most', 'customers', 'who', 'took', 'the', 'meeting', 'today', 'improperly', 'an', 'agent', 'for', 'gm', 'N', 'N', 'will', 'see', 'idle', 'its', '<unk>', '<unk>', 'program', 'figures', 'if', 'the']\n",
            "['fax', 'author', 'william', '<unk>', 'an', '<unk>', 'teacher', 'was', 'responsible', 'for', 'literature', 'requiring', 'the', '<unk>', 'of', 'the', 'market', \"'s\", 'faith', 'in', 'the', 'current', 'board', 'date', 'of', 'control', 'data', 'while', 'the', 'manhattan', 'borough', 'president', 'and', 'chief', 'partners', 'estimate', 'for', 'the', 'building', 'will', 'probably', 'expand', 'these', 'days', 'after', 'the', 'quake', 'he', 'has', 'pulled']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:09:50], Epoch [37/50], loss: 3.3840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:10:06], Epoch [38/50], loss: 3.3730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:10:21], Epoch [39/50], loss: 3.3674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:10:37], Epoch [40/50], loss: 3.3578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:10:53], Epoch [41/50], loss: 3.3503\n",
            "[Generated Sentences]\n",
            "['hope', 'who', 'are', 'wooden', 'equal', 'old', 'this', 'farmer', 'of', 'the', 'tank', 'in', '<unk>', 'capital', 'insight', 'inc.', '<unk>', 'the', 'orange', 'workers', 'for', 'corporations', \"'\", 'results', 'out', 'of', 'by', 'congress', 'rep.', 'bush', 'rejected', 'a', 'reputation', 'for', 'the', 'remaining', 'reliance', 'collection', 'agencies', 'are', 'and', 'the', 'flat', 'surface', 'where', 'he', 'has', 'an', 'enormous', '<unk>']\n",
            "['accused', 'congress', 'had', 'been', 'transformed', 'from', 'trails', 'pepsi', 'in', 'the', 'california', 'supreme', 'court', '<unk>', 'a', 'panamanian', 'diplomat', 'based', 'in', 'peru', 'named', 'luis', '&', 'telegraph', 'co.', 'and', 'dallas', 'says', 'he', 'hopes', 'all', 'of', 'these', 'numbers', 'wo', \"n't\", 'indicate', 'the', 'official', 'and', 'southeast', 'asian', 'trade', 'figures', 'without', 'excluding', 'bad', 'loan', 'price', 'for']\n",
            "['cities', 'competent', 'a', 'group', 'colgate', \"'s\", 'consortium', 'from', 'the', 'role', 'of', '<unk>', 'space', 'ltd.', 'worked', 'out', 'to', 'sustain', '<unk>', 'as', 'potential', 'targets', 'or', 'high', 'concentrated', 'in', 'building', 'the', 'soviet', 'side', 'and', '<unk>', 'cancers', 'are', 'being', 'announced', 'access', 'to', 'their', 'ability', 'to', 'restore', 'contact', 'with', '<unk>', 'over', 'the', 'next', 'N', 'was']\n",
            "['liberals', 'are', 'just', 'all', 'we', 'have', 'to', 'endure', 'supreme', 'court', 'or', 'pilots', 'were', 'looking', 'for', 'negligence', 'that', 'is', \"n't\", 'predicting', 'such', 'funds', 'do', \"n't\", 'have', 'the', 'automobile', 'and', 'just', 'walk', 'process', 'into', '<unk>', 'and', '<unk>', 'in', 'throwing', 'cold', 'feet', 'and', 'no', 'complaints', 'will', 'waive', 'these', 'certain', 'trigger', 'point', 'for', '<unk>']\n",
            "['onto', 'leaped', 'by', 'coming', 'than', 'the', '$', '<unk>', 'of', 'the', 'plant', 'in', 'the', 'years', 'saw', 'imports', 'posted', 'third-quarter', 'earnings', 'in', 'the', 'third', 'quarter', 'to', '$', 'N', 'million', 'or', '$', 'N', 'a', 'share', 'from', '$', 'N', 'million', 'or', 'N', 'cents', 'a', 'share', 'a', 'year', 'earlier', 'given', 'a', '$', 'N', 'million', 'oregon']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:11:09], Epoch [42/50], loss: 3.3445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:11:24], Epoch [43/50], loss: 3.3413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:11:40], Epoch [44/50], loss: 3.3407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:11:56], Epoch [45/50], loss: 3.3311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:12:12], Epoch [46/50], loss: 3.3250\n",
            "[Generated Sentences]\n",
            "['accessories', 'reason', 'lately', 'rampant', 'tom', '<unk>', 'suggested', 'that', 'kind', 'of', 'philosophy', 'programming', 'a', 'loose', 'state', 'court', 'complaining', 'that', 'they', 'would', 'find', 'action', 'in', 'the', 'country', 'that', 'are', 'often', 'protected', 'by', 'the', 'author', \"'s\", 'initiative', 'passed', 'in', 'another', 'important', 'ban', 'on', 'market', 'movements', 'curb', 'imports', 'appeal', 'were', 'common', 'along', 'says', 'mr.']\n",
            "['amoco', \"'s\", 'estimated', 'current', 'price', 'and', 'provide', 'a', 'c', 'investment', 'crop', 'in', 'the', 'mid-1970s', 'what', 'happens', 'because', 'it', 'just', 'two', 'is', \"n't\", 'a', 'lot', 'of', 'the', 'market', 'through', 'a', 'cd', 'following', 'an', 'new', 'zealand', 'government', 'of', 'N', 'employees', 'surveyed', 'are', \"n't\", 'being', 'made', 'available', 'systems', 'for', 'antitrust', 'big', 'in', 'seeking']\n",
            "['moral', 'urge', 'presentation', 'of', 'the', 'volatile', 'categories', 'and', '<unk>', 'ones', 'yesterday', 'via', '<unk>', 'hats', '<unk>', 'air', 'pollution', 'data', 'schedules', 'on', 'wood', 'boards', 'who', 'become', 'widely', 'fitness', 'and', 'chief', 'financial', 'economist', 'at', 'the', 'company', 'annual', 'sales', 'of', '$', 'N', 'billion', 'covering', 'boston', 'network', 'will', 'be', 'available', 'yet', 'priced', 'through', '<unk>', 'distribution']\n",
            "['determining', 'the', 'street', 'is', 'one', 'of', 'this', 'old', 'standard', 'city', 'danny', 'gulf', 'power', 'but', 'has', 'done', 'little', '<unk>', 'the', 'conventional', 'wisdom', 'soda', 'along', 'with', 'its', 'existing', 'artistic', 'entertainment', 'division', 'one', 'of', 'the', 'exempt', 'public', 'investment', 'in', 'houston', 'appeals', 'court', 'a', '<unk>', 'celebration', 'of', 'past', 'feet', 'but', 'a', '<unk>', 'near', 'but']\n",
            "['basis', 'unless', 'it', \"'s\", 'highly', 'buying', 'if', 'an', 'operator', 'and', 'its', 'functions', 'like', '<unk>', 'pharmaceuticals', 'systems', 'inc.', 'a', 'sentence', 'from', 'the', 'rating', 'seeking', '<unk>', 'authority', 'and', 'frequency', 'violations', 'in', 'the', 'meanwhile', 'the', '<unk>', 'ill.', 'automotive', 'parent', 'company', 'is', 'a', 'supplier', 'at', '<unk>', 'economically', '<unk>', 'software', 'strategies', 'that', 'may', 'store', 'the']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:12:28], Epoch [47/50], loss: 3.3223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:12:43], Epoch [48/50], loss: 3.3187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:12:59], Epoch [49/50], loss: 3.3153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:13:15], Epoch [50/50], loss: 3.3162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDhlrcENM4Dx"
      },
      "source": [
        "생성된 텍스트의 퀄리티는 어떤가요?\n",
        "\n",
        "앞으로 딥러닝 강의가 끝나면 자연어처리 강좌에서 텍스트 처리에 적합한 전처리 과정, 모델구조들을 본격적으로 배우시게 될것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ua-_6W2a5Lt"
      },
      "source": [
        "# References\n",
        "\n",
        "1. https://github.com/pytorch/examples/tree/master/word_language_model\n",
        "2. https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model"
      ]
    }
  ]
}