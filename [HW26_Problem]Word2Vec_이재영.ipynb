{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appletreeleaf/Study_Log/blob/NLP/%5BHW26_Problem%5DWord2Vec_%EC%9D%B4%EC%9E%AC%EC%98%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3FAK0fz1kOr"
      },
      "source": [
        "##**3. Word2Vec**\n",
        "1. 주어진 단어들을 word2vec 모델에 들어갈 수 있는 형태로 만듭니다.\n",
        "2. CBOW, Skip-gram 모델을 각각 구현합니다.\n",
        "3. 모델을 실제로 학습해보고 결과를 확인합니다.\n",
        "4. 산점도를 그려 단어들의 대략적인 위치를 확인해봅니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9FrxTPWIsct"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utBdiiW499DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26da30f-4c29-4e7f-b4c3-c3f9c34e18f6"
      },
      "source": [
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 9,599 kB of archives.\n",
            "After this operation, 29.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 fonts-nanum all 20180306-3 [9,599 kB]\n",
            "Fetched 9,599 kB in 2s (5,570 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 128126 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20180306-3_all.deb ...\n",
            "Unpacking fonts-nanum (20180306-3) ...\n",
            "Setting up fonts-nanum (20180306-3) ...\n",
            "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjroCdtwI9Rz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb581c8-d562-4e37-ebf1-5d2fe7753d44"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.6/465.6 KB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSP7aXfJIr3i"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from konlpy.tag import Mecab,Twitter,Okt,Kkma\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qugro74yJASr"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q36dfSRRJDtX"
      },
      "source": [
        "\n",
        "\n",
        "데이터를 확인하고 Word2Vec 형식에 맞게 전처리합니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLZ2f-lRJSus"
      },
      "source": [
        "train_data = [\n",
        "  \"정말 맛있습니다. 추천합니다.\",\n",
        "  \"기대했던 것보단 별로였네요.\",\n",
        "  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\n",
        "  \"완전 최고입니다! 재방문 의사 있습니다.\",\n",
        "  \"음식도 서비스도 다 만족스러웠습니다.\",\n",
        "  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\n",
        "  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\n",
        "  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\n",
        "  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\n",
        "  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"\n",
        "]\n",
        "\n",
        "test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vReElaFSLBYL"
      },
      "source": [
        "Tokenization과 vocab을 만드는 과정은 이전 실습과 유사합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tokenization"
      ],
      "metadata": {
        "id": "N_5SB_sX672p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTjlRzmWMDK_"
      },
      "source": [
        "tokenizer = Okt() # 품사태깅"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DTUsX672icp"
      },
      "source": [
        "def make_tokenized(data):\n",
        "  tokenized = []\n",
        "  for sent in tqdm(data):\n",
        "    tokens = tokenizer.morphs(sent, stem=True) # token을 품사 단위로 바꿔줌 ex) 맛있습니다 -> 맛있다\n",
        "    tokenized.append(tokens)\n",
        "\n",
        "  return tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-z0z6HD2rrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ea8ae7-82d7-4927-eec7-16652f5edb27"
      },
      "source": [
        "train_tokenized = make_tokenized(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:06<00:00,  1.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장들이 품사 단위로 나눠지는 것을 볼 수 있습니다.\n",
        "print(train_tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvM-OREl6eMP",
        "outputId": "7fb8a14a-95b0-423e-a6b5-a1f15824ec92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51exEpI0Mc3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2e8e79-57e6-4a36-c6a3-62dd1e9e3ccd"
      },
      "source": [
        "word_count = defaultdict(int)\n",
        "\n",
        "for tokens in tqdm(train_tokenized):\n",
        "  for token in tokens:\n",
        "    word_count[token] += 1 # token의 빈도를 측정"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 73326.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_count.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQMF-_ab7rzV",
        "outputId": "f5bed1a2-d86d-4abb-efde-3db5bf129938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('정말', 1), ('맛있다', 1), ('.', 14), ('추천', 1), ('하다', 2), ('기대하다', 1), ('것', 1), ('보단', 1), ('별로', 3), ('이다', 4), ('다', 3), ('좋다', 4), ('가격', 1), ('이', 3), ('너무', 3), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('방문', 2), ('의사', 1), ('있다', 1), ('음식', 3), ('도', 7), ('서비스', 3), ('만족스럽다', 1), ('위생', 2), ('상태', 1), ('가', 1), ('좀', 2), ('더', 2), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('에', 2), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('조금', 2), ('신경', 1), ('써다', 1), ('불쾌하다', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyvHAMAnMh1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9d7268-bb10-4fb3-a645-3eb5e2ffd7fc"
      },
      "source": [
        "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True) #tuple\n",
        "print(list(word_count))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', 14), ('도', 7), ('이다', 4), ('좋다', 4), ('별로', 3), ('다', 3), ('이', 3), ('너무', 3), ('음식', 3), ('서비스', 3), ('하다', 2), ('방문', 2), ('위생', 2), ('좀', 2), ('더', 2), ('에', 2), ('조금', 2), ('정말', 1), ('맛있다', 1), ('추천', 1), ('기대하다', 1), ('것', 1), ('보단', 1), ('가격', 1), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('의사', 1), ('있다', 1), ('만족스럽다', 1), ('상태', 1), ('가', 1), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('신경', 1), ('써다', 1), ('불쾌하다', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaK_i3zL2vO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae8478a0-566f-4e5a-e386-156eab54ee4e"
      },
      "source": [
        "w2i = {} # tuple to dict\n",
        "for pair in tqdm(word_count):\n",
        "  if pair[0] not in w2i:    # (word, frequency)이므로 pair[0] == word\n",
        "    w2i[pair[0]] = len(w2i) # 헤당 word가 word2idx dict에 없다면, update하고 index부여   *len(w2i)는 0,1,2.... 즉 0부터 idx부여\n",
        "\n",
        "i2w={v:k for k,v in w2i.items()} # 위의 역과정"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:00<00:00, 305410.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcm_L4iJBufO"
      },
      "source": [
        "### 다음은 Word2Vec을 학습시키는 대표적인 방법인 Skipgram과 CBoW를 다룹니다.\n",
        "\n",
        "* CboW는 주변단어를 이용해, 주어진 단어를 예측하는 방법입니다.\n",
        "* Skipgram은 중심 단어를 이용하여 주변 단어를 예측하는 방법입니다.\n",
        "* 즉 데이터셋을 구성할때, input x 와 target y를 어떻게 설정하는지에 차이가 있습니다.\n",
        "\n",
        "참고자료\n",
        "\n",
        "* https://simonezz.tistory.com/35\n",
        "\n",
        "* https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXA5zaPPM3Wd"
      },
      "source": [
        "실제 모델에 들어가기 위한 input을 만들기 위해 `Dataset` 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s47ssyVt89t1"
      },
      "source": [
        "class CBOWDataset(Dataset):\n",
        "  def __init__(self, train_tokenized, window_size=2):\n",
        "    self.x = [] # input word\n",
        "    self.y = [] # target word\n",
        "\n",
        "    for tokens in tqdm(train_tokenized):\n",
        "      token_ids = [w2i[token] for token in tokens]    # word에 대응되는 index값들이 들어있음\n",
        "      for i, id in enumerate(token_ids): # i=단어위치, id=단어의 정수 인덱스값\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\n",
        "          ############################ ANSWER HERE ################################\n",
        "          # TODO 1: insert tokens for input self.x\n",
        "          # TODO 2: insert tokens for targets self.y\n",
        "          self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+1+window_size])\n",
        "          self.y.append(id)\n",
        "          #########################################################################\n",
        "\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size)\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvInhQ33AMJv"
      },
      "source": [
        "class SkipGramDataset(Dataset):\n",
        "  def __init__(self, train_tokenized, window_size=2):\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "\n",
        "    for tokens in tqdm(train_tokenized):\n",
        "      token_ids = [w2i[token] for token in tokens]\n",
        "      for i, id in enumerate(token_ids):\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\n",
        "            ############################ ANSWER HERE ################################\n",
        "          # TODO 1: insert tokens for input self.x\n",
        "          # TODO 2: insert tokens for targets self.y\n",
        "          self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1]) # append랑 +=한거랑 차이가 있나?\n",
        "          self.x += [id] * 2 * window_size\n",
        "          #########################################################################\n",
        "\n",
        "\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수)\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyAGV5IUUba0"
      },
      "source": [
        "각 모델에 맞는 `Dataset` 객체를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ep7Hm6oBWyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae810f6f-a736-4145-fa4d-976e0e584549"
      },
      "source": [
        "cbow_set = CBOWDataset(train_tokenized)\n",
        "skipgram_set = SkipGramDataset(train_tokenized)\n",
        "print(list(skipgram_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 24442.33it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 41241.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(tensor(0), tensor(17)), (tensor(0), tensor(18)), (tensor(0), tensor(19)), (tensor(0), tensor(10)), (tensor(19), tensor(18)), (tensor(19), tensor(0)), (tensor(19), tensor(10)), (tensor(19), tensor(0)), (tensor(22), tensor(20)), (tensor(22), tensor(21)), (tensor(22), tensor(4)), (tensor(22), tensor(2)), (tensor(4), tensor(21)), (tensor(4), tensor(22)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(23), tensor(5)), (tensor(23), tensor(3)), (tensor(23), tensor(6)), (tensor(23), tensor(7)), (tensor(6), tensor(3)), (tensor(6), tensor(23)), (tensor(6), tensor(7)), (tensor(6), tensor(24)), (tensor(7), tensor(23)), (tensor(7), tensor(6)), (tensor(7), tensor(24)), (tensor(7), tensor(25)), (tensor(24), tensor(6)), (tensor(24), tensor(7)), (tensor(24), tensor(25)), (tensor(24), tensor(26)), (tensor(25), tensor(7)), (tensor(25), tensor(24)), (tensor(25), tensor(26)), (tensor(25), tensor(27)), (tensor(26), tensor(24)), (tensor(26), tensor(25)), (tensor(26), tensor(27)), (tensor(26), tensor(28)), (tensor(27), tensor(25)), (tensor(27), tensor(26)), (tensor(27), tensor(28)), (tensor(27), tensor(6)), (tensor(28), tensor(26)), (tensor(28), tensor(27)), (tensor(28), tensor(6)), (tensor(28), tensor(29)), (tensor(6), tensor(27)), (tensor(6), tensor(28)), (tensor(6), tensor(29)), (tensor(6), tensor(30)), (tensor(29), tensor(28)), (tensor(29), tensor(6)), (tensor(29), tensor(30)), (tensor(29), tensor(31)), (tensor(30), tensor(6)), (tensor(30), tensor(29)), (tensor(30), tensor(31)), (tensor(30), tensor(0)), (tensor(2), tensor(32)), (tensor(2), tensor(33)), (tensor(2), tensor(34)), (tensor(2), tensor(35)), (tensor(34), tensor(33)), (tensor(34), tensor(2)), (tensor(34), tensor(35)), (tensor(34), tensor(11)), (tensor(35), tensor(2)), (tensor(35), tensor(34)), (tensor(35), tensor(11)), (tensor(35), tensor(36)), (tensor(11), tensor(34)), (tensor(11), tensor(35)), (tensor(11), tensor(36)), (tensor(11), tensor(37)), (tensor(36), tensor(35)), (tensor(36), tensor(11)), (tensor(36), tensor(37)), (tensor(36), tensor(0)), (tensor(9), tensor(8)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(38)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(38)), (tensor(5), tensor(0)), (tensor(40), tensor(12)), (tensor(40), tensor(39)), (tensor(40), tensor(13)), (tensor(40), tensor(4)), (tensor(13), tensor(39)), (tensor(13), tensor(40)), (tensor(13), tensor(4)), (tensor(13), tensor(2)), (tensor(4), tensor(40)), (tensor(4), tensor(13)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(2), tensor(13)), (tensor(2), tensor(4)), (tensor(2), tensor(0)), (tensor(2), tensor(13)), (tensor(0), tensor(4)), (tensor(0), tensor(2)), (tensor(0), tensor(13)), (tensor(0), tensor(14)), (tensor(13), tensor(2)), (tensor(13), tensor(0)), (tensor(13), tensor(14)), (tensor(13), tensor(41)), (tensor(14), tensor(0)), (tensor(14), tensor(13)), (tensor(14), tensor(41)), (tensor(14), tensor(42)), (tensor(41), tensor(13)), (tensor(41), tensor(14)), (tensor(41), tensor(42)), (tensor(41), tensor(43)), (tensor(42), tensor(14)), (tensor(42), tensor(41)), (tensor(42), tensor(43)), (tensor(42), tensor(44)), (tensor(43), tensor(41)), (tensor(43), tensor(42)), (tensor(43), tensor(44)), (tensor(43), tensor(0)), (tensor(3), tensor(45)), (tensor(3), tensor(1)), (tensor(3), tensor(46)), (tensor(3), tensor(47)), (tensor(46), tensor(1)), (tensor(46), tensor(3)), (tensor(46), tensor(47)), (tensor(46), tensor(9)), (tensor(47), tensor(3)), (tensor(47), tensor(46)), (tensor(47), tensor(9)), (tensor(47), tensor(1)), (tensor(9), tensor(46)), (tensor(9), tensor(47)), (tensor(9), tensor(1)), (tensor(9), tensor(7)), (tensor(1), tensor(47)), (tensor(1), tensor(9)), (tensor(1), tensor(7)), (tensor(1), tensor(48)), (tensor(7), tensor(9)), (tensor(7), tensor(1)), (tensor(7), tensor(48)), (tensor(7), tensor(0)), (tensor(11), tensor(49)), (tensor(11), tensor(15)), (tensor(11), tensor(10)), (tensor(11), tensor(8)), (tensor(10), tensor(15)), (tensor(10), tensor(11)), (tensor(10), tensor(8)), (tensor(10), tensor(1)), (tensor(8), tensor(11)), (tensor(8), tensor(10)), (tensor(8), tensor(1)), (tensor(8), tensor(50)), (tensor(1), tensor(10)), (tensor(1), tensor(8)), (tensor(1), tensor(50)), (tensor(1), tensor(1)), (tensor(50), tensor(8)), (tensor(50), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(9)), (tensor(1), tensor(1)), (tensor(1), tensor(50)), (tensor(1), tensor(9)), (tensor(1), tensor(1)), (tensor(9), tensor(50)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(3)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(3)), (tensor(5), tensor(0)), (tensor(53), tensor(51)), (tensor(53), tensor(52)), (tensor(53), tensor(8)), (tensor(53), tensor(6)), (tensor(8), tensor(52)), (tensor(8), tensor(53)), (tensor(8), tensor(6)), (tensor(8), tensor(7)), (tensor(6), tensor(53)), (tensor(6), tensor(8)), (tensor(6), tensor(7)), (tensor(6), tensor(54)), (tensor(7), tensor(8)), (tensor(7), tensor(6)), (tensor(7), tensor(54)), (tensor(7), tensor(0)), (tensor(54), tensor(6)), (tensor(54), tensor(7)), (tensor(54), tensor(0)), (tensor(54), tensor(55)), (tensor(0), tensor(7)), (tensor(0), tensor(54)), (tensor(0), tensor(55)), (tensor(0), tensor(56)), (tensor(55), tensor(54)), (tensor(55), tensor(0)), (tensor(55), tensor(56)), (tensor(55), tensor(4)), (tensor(56), tensor(0)), (tensor(56), tensor(55)), (tensor(56), tensor(4)), (tensor(56), tensor(2)), (tensor(4), tensor(55)), (tensor(4), tensor(56)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(16), tensor(12)), (tensor(16), tensor(15)), (tensor(16), tensor(14)), (tensor(16), tensor(57)), (tensor(14), tensor(15)), (tensor(14), tensor(16)), (tensor(14), tensor(57)), (tensor(14), tensor(58)), (tensor(57), tensor(16)), (tensor(57), tensor(14)), (tensor(57), tensor(58)), (tensor(57), tensor(3)), (tensor(58), tensor(14)), (tensor(58), tensor(57)), (tensor(58), tensor(3)), (tensor(58), tensor(0)), (tensor(3), tensor(57)), (tensor(3), tensor(58)), (tensor(3), tensor(0)), (tensor(3), tensor(16)), (tensor(0), tensor(58)), (tensor(0), tensor(3)), (tensor(0), tensor(16)), (tensor(0), tensor(59)), (tensor(16), tensor(3)), (tensor(16), tensor(0)), (tensor(16), tensor(59)), (tensor(16), tensor(0))]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QSo73PoRyd9"
      },
      "source": [
        "### **모델 Class 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnnk44R6R28x"
      },
      "source": [
        "차례대로 두 가지 Word2Vec 모델을 구현합니다.  \n",
        "\n",
        "\n",
        "*   `self.embedding`: `vocab_size` 크기의 one-hot vector를 특정 크기의 `dim` 차원으로 embedding 시키는 layer.\n",
        "*   `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.ones(2,3)\n",
        "q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAfOwSUfHCtG",
        "outputId": "a21f9425-ba13-4a71-9547-8c435091daa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_HP1ISq5CWv"
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super(CBOW, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True) # vocab_size 크기의 one-hot vector를 우리가 지정한 dimension으로 embedding\n",
        "    self.linear = nn.Linear(dim, vocab_size)\n",
        "\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
        "  def forward(self, x):  # x: (B, 2W)\n",
        "    embeddings = self.embedding(x)  # (B, 2W, d_w)\n",
        "    ##### 위에서 아래로 dim이 바뀐 이유를 모르겠네\n",
        "    embeddings = torch.sum(embeddings, dim=1)  # (B, d_w)\n",
        "    output = self.linear(embeddings)  # (B, V)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQAUApww68MJ"
      },
      "source": [
        "class SkipGram(nn.Module):\n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super(SkipGram, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
        "    self.linear = nn.Linear(dim, vocab_size)\n",
        "\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
        "  def forward(self, x): # x: (B)\n",
        "    embeddings = self.embedding(x)  # (B, d_w)\n",
        "    output = self.linear(embeddings)  # (B, V)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58cJalkDWYMT"
      },
      "source": [
        "두 가지 모델을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWUXEi8WeM-"
      },
      "source": [
        "cbow = CBOW(vocab_size=len(w2i), dim=256) # vocab_size = unique word = w2i dictionary의 단어 수(크기,길이이)\n",
        "skipgram = SkipGram(vocab_size=len(w2i), dim=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxP7qdtNWil1"
      },
      "source": [
        "### **모델 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVggZrQ4WpBS"
      },
      "source": [
        "다음과 같이 hyperparamter를 세팅하고 `DataLoader` 객체를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygVdz5rSBeNu"
      },
      "source": [
        "batch_size=4\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 5\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\n",
        "skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekixqKB3X5C1"
      },
      "source": [
        "첫번째로 CBOW 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d95qR7oC822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3245fb27-9a71-4d07-cd74-7be7d9231e79"
      },
      "source": [
        "cbow.train()\n",
        "cbow = cbow.to(device) # to gpu\n",
        "optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate) #optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(1, num_epochs+1):\n",
        "  print(\"#\" * 50)\n",
        "  print(f\"Epoch: {e}\")\n",
        "  for batch in tqdm(cbow_loader):\n",
        "    x, y = batch\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\n",
        "    output = cbow(x)  # (B, V)\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss = loss_function(output, y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    print(f\"Train loss: {loss.item()}\")\n",
        "\n",
        "print(\"Finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:03<00:00,  5.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.312560081481934\n",
            "Train loss: 4.706026554107666\n",
            "Train loss: 5.215273380279541\n",
            "Train loss: 4.611527442932129\n",
            "Train loss: 5.4254045486450195\n",
            "Train loss: 5.705330848693848\n",
            "Train loss: 4.925873279571533\n",
            "Train loss: 4.497958183288574\n",
            "Train loss: 4.609296798706055\n",
            "Train loss: 4.395036220550537\n",
            "Train loss: 4.352705001831055\n",
            "Train loss: 5.094509124755859\n",
            "Train loss: 3.2858364582061768\n",
            "Train loss: 4.741290092468262\n",
            "Train loss: 6.013391971588135\n",
            "Train loss: 4.65230131149292\n",
            "##################################################\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 610.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.138981819152832\n",
            "Train loss: 4.547187328338623\n",
            "Train loss: 5.084856033325195\n",
            "Train loss: 4.486894607543945\n",
            "Train loss: 5.283964157104492\n",
            "Train loss: 5.450082302093506\n",
            "Train loss: 4.711617469787598\n",
            "Train loss: 4.33994197845459\n",
            "Train loss: 4.490058898925781\n",
            "Train loss: 4.212679386138916\n",
            "Train loss: 4.203240871429443\n",
            "Train loss: 4.767696380615234\n",
            "Train loss: 3.175046443939209\n",
            "Train loss: 4.593456268310547\n",
            "Train loss: 5.8508076667785645\n",
            "Train loss: 4.491775035858154\n",
            "##################################################\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 600.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.9697749614715576\n",
            "Train loss: 4.393424034118652\n",
            "Train loss: 4.956082344055176\n",
            "Train loss: 4.365739345550537\n",
            "Train loss: 5.1449174880981445\n",
            "Train loss: 5.2004523277282715\n",
            "Train loss: 4.506065368652344\n",
            "Train loss: 4.1862335205078125\n",
            "Train loss: 4.374305725097656\n",
            "Train loss: 4.0347700119018555\n",
            "Train loss: 4.058470249176025\n",
            "Train loss: 4.450901985168457\n",
            "Train loss: 3.0661864280700684\n",
            "Train loss: 4.454350471496582\n",
            "Train loss: 5.690592288970947\n",
            "Train loss: 4.340354919433594\n",
            "##################################################\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.804551362991333\n",
            "Train loss: 4.244121074676514\n",
            "Train loss: 4.828907012939453\n",
            "Train loss: 4.247568130493164\n",
            "Train loss: 5.008188247680664\n",
            "Train loss: 4.956947326660156\n",
            "Train loss: 4.30853796005249\n",
            "Train loss: 4.036670684814453\n",
            "Train loss: 4.262054920196533\n",
            "Train loss: 3.8616323471069336\n",
            "Train loss: 3.9187569618225098\n",
            "Train loss: 4.145650863647461\n",
            "Train loss: 2.9592394828796387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 51.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.3225812911987305\n",
            "Train loss: 5.532642841339111\n",
            "Train loss: 4.196422576904297\n",
            "##################################################\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 540.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.6430931091308594\n",
            "Train loss: 4.098860740661621\n",
            "Train loss: 4.703300476074219\n",
            "Train loss: 4.132038593292236\n",
            "Train loss: 4.87372350692749\n",
            "Train loss: 4.72025728225708\n",
            "Train loss: 4.118520259857178\n",
            "Train loss: 3.891141891479492\n",
            "Train loss: 4.153350353240967\n",
            "Train loss: 3.6936380863189697\n",
            "Train loss: 3.7845304012298584\n",
            "Train loss: 3.8537731170654297\n",
            "Train loss: 2.854238271713257\n",
            "Train loss: 4.197100639343262\n",
            "Train loss: 5.376893997192383\n",
            "Train loss: 4.058853626251221\n",
            "Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDahBf6IX4py"
      },
      "source": [
        "다음으로 Skip-gram 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJxGEusqFV5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c485493-c327-4716-efdd-b3447cbe16b4"
      },
      "source": [
        "skipgram.train()\n",
        "skipgram = skipgram.to(device)\n",
        "optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "for e in range(1, num_epochs+1):\n",
        "  print(\"#\" * 50)\n",
        "  print(f\"Epoch: {e}\")\n",
        "  for batch in tqdm(skipgram_loader):\n",
        "    x, y = batch\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\n",
        "    output = skipgram(x)  # (B, V)\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss = loss_function(output, y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "  print(f\"Train loss: {loss.item()}\")\n",
        "\n",
        "print(\"Finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 986.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.7847466468811035\n",
            "##################################################\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 1098.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.732769966125488\n",
            "##################################################\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 1048.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.681147575378418\n",
            "##################################################\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 1074.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.629892826080322\n",
            "##################################################\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 1070.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.579020023345947\n",
            "Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi0sbHV6dEOR"
      },
      "source": [
        "### **테스트**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGarLWxXeJvz"
      },
      "source": [
        "학습된 각 모델을 이용하여 test 단어들의 word embedding을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A1wrl-L_RjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b329bece-5fdb-4cdb-a6fa-a9728b85603d"
      },
      "source": [
        "for word in test_words:\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device) # test_words의 word들을 index값으로 변환 후 torch.LongTensor 변환 후 gpu로 보냄냄\n",
        "  emb = cbow.embedding(input_id)\n",
        "\n",
        "  print(f\"Word: {word}\")\n",
        "  print(emb.squeeze(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: 음식\n",
            "tensor([ 0.6124, -0.0123,  1.1840, -0.1200,  0.0866,  0.0493,  1.3134, -1.3995,\n",
            "         1.0501, -1.0432,  0.0651,  0.6847,  0.0682, -0.3326, -0.5170, -0.7117,\n",
            "         0.2705,  0.5727,  1.4585, -0.1662,  2.0853, -1.0293, -0.0393,  1.7251,\n",
            "         0.6467,  0.6998,  0.7572, -0.4284,  1.0301, -0.0747, -2.4505,  0.8699,\n",
            "         0.6280, -0.3213, -0.4516,  0.0640,  0.5933, -1.2895,  2.3883,  0.2088,\n",
            "        -0.6019, -1.5515, -1.7922, -1.2726, -2.1368, -1.6793,  0.2320,  0.2062,\n",
            "         0.3506,  0.5164,  0.0775,  0.7151, -1.3907, -0.5931, -0.2518,  1.4562,\n",
            "         1.8961,  0.4993, -0.9746,  1.8871,  0.7030,  0.0398,  0.5138, -0.5946,\n",
            "         1.8336,  1.8695,  0.3665,  1.1417,  0.5302, -1.1870,  0.1154,  0.3861,\n",
            "         1.0715,  1.0872,  0.4305, -0.0846,  0.6982,  0.3231, -0.1420,  0.1782,\n",
            "        -1.8644,  0.0258,  1.0901,  0.3801, -0.3710,  1.3006,  0.7586, -1.0695,\n",
            "        -1.0924,  1.2853,  1.3836, -1.3088,  1.2804, -0.0943, -0.9306, -1.7242,\n",
            "        -0.0034,  0.9482,  0.1237, -1.2607,  2.2190,  0.2584, -0.1322, -0.6639,\n",
            "        -1.7966,  0.7250,  0.1797, -0.2176, -0.9277,  0.4018, -0.1141,  1.1183,\n",
            "        -1.5543,  0.2213,  0.2058,  0.1989, -0.9954, -1.1365,  0.8730, -0.2531,\n",
            "         0.1474, -1.7358,  0.4942, -0.3369, -2.0481, -0.3342, -0.2960, -2.6195,\n",
            "         0.7327, -0.1543,  0.8492,  0.4881, -0.2824,  1.8693, -1.6851,  0.4316,\n",
            "         0.0365,  1.0044,  1.2636,  1.0187,  1.6836, -1.2249, -0.8703,  0.9711,\n",
            "         0.9166,  0.1828,  0.4799,  0.5952,  0.0335,  1.2790,  1.7159,  0.6396,\n",
            "         1.5082, -0.3297, -0.3626, -1.4912,  0.7160,  0.2628, -1.4422,  0.0666,\n",
            "         0.6891,  0.4163, -0.0721,  0.8769, -1.1181, -0.2318,  0.4479, -0.3906,\n",
            "        -1.2777, -1.2491,  0.0932,  0.8831,  0.2920,  0.4778,  2.2568,  0.2104,\n",
            "         0.8833, -2.0064, -0.7343,  1.3065,  1.7742, -1.1557,  0.0926, -0.5544,\n",
            "        -0.1648,  0.7307,  1.6525, -0.9356, -0.4005, -0.5885,  0.4546, -1.7168,\n",
            "        -0.6665,  0.4383,  0.8533,  0.7910, -0.5437, -1.3048, -1.0910, -0.5215,\n",
            "         1.0346,  0.1877,  0.6868, -1.5426,  0.9094, -0.3448,  0.4358,  1.0577,\n",
            "        -0.5819, -0.4074, -0.6519, -0.4976,  0.7976,  0.4523,  1.0158, -1.3791,\n",
            "         2.3394, -2.5681,  0.2060, -0.6262,  0.7355,  0.3807, -0.6412, -0.6197,\n",
            "        -0.4613,  1.2376, -0.2339, -0.9493,  0.8112,  0.8449, -0.6797,  1.0475,\n",
            "        -1.1668, -0.2992,  0.5679,  0.9264,  1.4992,  1.4552,  0.5452,  0.6686,\n",
            "        -1.1319,  0.5948, -0.8396,  1.5495, -1.6960,  0.8491,  1.6502, -0.7560,\n",
            "        -0.4183,  0.8810, -1.5315,  1.5031,  0.9458, -0.8277,  1.9630, -0.5627],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 맛\n",
            "tensor([-9.7293e-01,  3.0379e-01, -3.4487e-01,  9.9038e-01,  6.8579e-02,\n",
            "         7.6747e-01, -1.5753e-01,  1.2703e+00, -5.6862e-01,  7.7080e-02,\n",
            "        -1.2305e+00, -1.3082e-01, -1.5062e-02,  1.5715e+00, -9.8400e-01,\n",
            "        -6.7139e-01, -5.3819e-01, -5.9178e-01, -6.2585e-01, -3.9519e-01,\n",
            "         1.8291e+00, -3.7619e-01, -8.0558e-01, -6.2173e-01,  4.7248e-01,\n",
            "         9.4045e-02, -7.0826e-01, -8.4112e-01,  9.0804e-01, -5.4572e-01,\n",
            "        -1.0072e+00, -4.7249e-01,  1.2729e+00, -5.3364e-01, -7.2761e-01,\n",
            "         8.9682e-01,  4.2025e-01,  3.9392e-01,  6.3120e-01,  8.4215e-01,\n",
            "         2.3570e-01, -3.4158e-01,  3.2588e-01, -1.5660e+00, -2.7185e-01,\n",
            "         1.3346e+00,  2.2434e-01, -1.2243e+00,  7.6127e-01, -2.5746e-01,\n",
            "         7.0748e-01,  4.7695e-01, -1.1567e+00, -3.6621e-01,  9.1925e-01,\n",
            "         7.0256e-01,  9.8973e-01,  7.6811e-02,  1.8946e-01,  7.4352e-01,\n",
            "        -7.8694e-01, -1.9925e-01,  1.9045e+00,  2.3242e-02,  3.2522e-02,\n",
            "         1.0497e+00, -2.1145e-01, -1.4749e+00, -6.1194e-01,  1.1002e+00,\n",
            "        -1.5102e-01, -8.8539e-01,  1.1254e+00,  1.8441e+00,  2.4322e-01,\n",
            "         1.3863e+00, -8.6820e-01, -1.6616e-01, -5.9246e-01,  1.8655e-01,\n",
            "        -1.2552e+00, -5.5979e-01,  2.1659e-01,  6.9709e-02, -2.4055e-02,\n",
            "         1.0004e+00, -9.3402e-01,  9.2968e-01, -9.3341e-01, -2.7731e+00,\n",
            "         1.6933e+00,  3.4571e-01,  2.0421e-01,  1.8820e+00,  5.0048e-01,\n",
            "        -9.1325e-01,  4.0372e-02, -8.7241e-01,  1.3780e+00, -4.1766e-01,\n",
            "         5.4860e-01,  9.8859e-01, -1.0801e+00,  4.9457e-01,  1.5956e+00,\n",
            "        -1.5696e+00,  6.7226e-01, -6.4748e-01, -9.5569e-01,  3.7743e-01,\n",
            "         7.8353e-01, -2.0599e-01,  6.2665e-01, -4.5072e-01, -9.3037e-01,\n",
            "        -2.3783e+00,  1.1981e+00, -1.0236e+00,  1.3576e+00,  1.1318e+00,\n",
            "         1.3533e-01, -1.1362e-01,  4.4299e-01,  1.9875e-01, -2.8220e-01,\n",
            "         8.1617e-03, -1.0966e+00,  1.9328e+00, -9.5165e-01, -7.4398e-01,\n",
            "         1.4203e+00, -2.0677e-01,  1.7996e+00,  7.0678e-01,  4.0038e-01,\n",
            "        -5.3184e-01,  6.4595e-01,  5.1113e-02,  1.9767e-02, -2.4541e-02,\n",
            "        -2.9216e-01, -7.9413e-01,  1.8693e+00,  1.2249e-01,  3.1173e-02,\n",
            "         1.7047e-01,  4.3996e-01, -1.5553e+00, -3.2967e-01,  1.8864e+00,\n",
            "         2.5984e-01,  2.8892e-03, -9.1194e-01,  1.4841e+00, -7.5240e-01,\n",
            "         6.0745e-01,  4.5009e-01,  1.7378e+00,  3.5153e-01,  5.5854e-01,\n",
            "        -8.7144e-01,  1.6454e-01, -8.3773e-01,  1.3957e+00,  5.0506e-02,\n",
            "         1.2903e+00,  6.6280e-01, -1.8929e-03, -1.6767e+00, -7.0704e-01,\n",
            "        -5.4632e-01,  2.8675e+00, -1.0009e+00,  7.7425e-01,  7.4526e-01,\n",
            "         8.1387e-01,  1.5102e+00, -1.1260e+00, -4.3331e-01, -1.5675e+00,\n",
            "         1.0389e+00, -2.7395e-01, -1.1909e-02,  9.0390e-01, -3.8805e-01,\n",
            "         6.7175e-01,  1.9157e-01, -4.5691e-01, -4.7895e-01,  9.7768e-01,\n",
            "        -3.3217e-01, -1.3289e+00, -2.2845e-01, -1.2751e+00,  2.0333e+00,\n",
            "        -9.1058e-01, -5.5356e-01,  1.5698e+00, -6.2067e-01, -7.5139e-01,\n",
            "         1.2127e-01, -1.1685e+00,  2.3337e+00,  5.8692e-01,  8.3841e-01,\n",
            "         6.7886e-01, -5.0997e-01,  4.9269e-01,  3.4282e-01, -7.8172e-01,\n",
            "        -1.2303e-01,  6.5381e-01, -3.0800e-01,  5.1781e-01,  8.2237e-01,\n",
            "        -4.7270e-01, -1.4500e+00,  6.9783e-01, -2.2291e+00,  1.4783e+00,\n",
            "         8.2686e-01,  1.4563e+00, -2.0969e+00, -1.1923e+00,  4.1785e-01,\n",
            "        -6.7329e-01,  3.7846e-01,  9.9719e-01,  2.0383e+00, -1.0006e+00,\n",
            "        -2.1715e-01, -5.7877e-01,  1.3752e+00, -2.8148e-01, -2.0042e-01,\n",
            "        -6.6506e-01,  8.9003e-01, -4.7032e-01, -1.6944e-01, -2.5685e-01,\n",
            "        -1.0695e+00, -7.3631e-01,  2.0515e+00, -2.9848e-02, -1.6887e-01,\n",
            "         1.0634e+00, -1.6524e+00, -5.7224e-02, -2.7856e+00, -1.4896e+00,\n",
            "         1.8817e+00, -1.8872e-01, -1.4775e+00,  6.2123e-01, -2.8147e-02,\n",
            "        -1.8109e+00], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 서비스\n",
            "tensor([-1.2297,  0.1527,  0.6887,  0.5332,  0.0873,  0.1095,  1.3205,  2.9106,\n",
            "         1.1506, -0.5558, -0.2883, -0.8217,  0.2090,  0.7734, -1.5554,  1.0223,\n",
            "        -0.7921,  1.1697, -0.1330, -0.8238,  0.0560,  0.3092,  0.0376, -0.2120,\n",
            "         1.9715, -1.1881,  0.9303,  0.3580,  1.1569,  0.9845,  0.0178, -0.4275,\n",
            "        -0.2431, -0.1700,  0.3100,  0.5942,  0.1650, -0.2347,  0.5395, -0.1565,\n",
            "        -1.3228,  0.2653,  0.4661,  0.7500,  0.6468,  0.5442,  1.0378, -0.8243,\n",
            "         1.7950, -1.3654,  0.5149,  0.3425, -0.8793,  0.1883, -0.8644, -0.4463,\n",
            "        -0.7117, -0.3443, -1.6102, -0.7773,  0.6278,  0.8627, -0.6714, -0.0446,\n",
            "         0.3846,  0.2255,  1.9796,  0.0150,  0.6456,  0.2067,  0.8815, -1.8745,\n",
            "        -1.6352, -1.2958, -0.1002,  0.2677, -1.9479,  1.1059, -0.5811,  1.2208,\n",
            "        -1.2644, -1.0106,  1.4630,  0.8830,  1.2485, -0.1516, -0.7871,  0.4341,\n",
            "        -0.8365,  1.8976,  1.7783,  0.5926, -0.4397, -1.3526,  0.7189, -1.2498,\n",
            "        -0.3098,  0.0998,  0.4722,  0.2124, -0.4008, -0.5204,  0.5776, -0.4213,\n",
            "         0.2757, -1.1616,  1.0091,  2.0486, -2.2340,  1.1894, -0.6240,  0.0163,\n",
            "         0.9066, -0.6631,  2.5691,  1.0029,  0.7151,  0.3862,  0.6544, -1.4213,\n",
            "         0.3262, -0.6161, -0.4635,  0.8910, -1.1212,  0.6950,  0.4768,  0.6912,\n",
            "        -0.8677, -1.7839, -1.0013,  0.1766,  0.4255,  1.3013, -1.0192, -0.7556,\n",
            "        -0.3832,  0.5231,  1.3778, -1.1999,  2.3186,  1.6932, -0.7027,  1.8520,\n",
            "        -0.4435, -0.2483, -0.2449,  2.5935, -0.3139, -0.5464,  0.3927,  1.0161,\n",
            "         0.0689,  1.0755,  0.3390, -0.9075, -1.2204, -0.7350,  0.8696,  0.0106,\n",
            "         1.0402,  0.8652,  1.2133,  1.2635, -0.1153, -1.9028, -0.5957, -0.3624,\n",
            "        -0.2604,  0.1928, -0.8806,  0.3306,  0.3630,  0.4118,  0.1369, -0.6681,\n",
            "        -1.3353,  0.5116, -0.8638,  1.5591, -1.4884, -1.2504,  2.5529, -0.9550,\n",
            "        -0.8150,  0.6247,  0.8774, -0.0152, -0.7885,  1.4320,  0.7092, -0.4172,\n",
            "         0.6224,  1.7186, -0.4840,  1.5126,  0.9349,  0.5518,  0.7223, -0.5721,\n",
            "         0.0988,  1.3392, -0.4399, -1.4106, -1.2825, -0.4534,  1.6170, -0.0210,\n",
            "        -0.3879,  0.8086,  0.4635,  0.6823,  0.3332,  1.7730, -0.8121, -0.3247,\n",
            "         0.7655, -0.8359,  0.1025, -0.8446, -0.6898,  1.1653, -0.9028, -0.7694,\n",
            "        -1.6723,  1.5005,  0.0966,  0.1156,  0.2828,  0.4179, -0.5394,  1.2274,\n",
            "         1.7314,  1.7940, -0.1847,  1.1294,  0.2435, -0.5302, -0.8458, -0.2059,\n",
            "        -0.8668,  0.6828,  1.4004, -0.0553,  0.5312, -0.5555,  1.8872, -0.2163,\n",
            "        -0.0995, -0.0189,  1.4288, -1.7000,  0.6800, -0.7101,  1.2962,  0.9671],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 위생\n",
            "tensor([ 0.2767, -0.5239, -1.6338, -0.2561, -0.0128,  0.8775,  1.4114, -0.4042,\n",
            "         0.0702,  0.3977,  1.1692, -0.2697,  1.9898, -0.4321,  0.0661, -1.1162,\n",
            "         0.3780,  0.2985,  0.8475, -0.5445, -0.7501, -0.0882, -1.6052, -0.5329,\n",
            "         0.2294, -0.1113,  0.9994,  1.0569, -0.2821, -1.4576, -0.2228,  0.1572,\n",
            "         1.6096, -1.2071, -0.5547,  1.2463, -1.6699,  0.5365,  0.2698, -1.1278,\n",
            "         1.4027, -0.7659,  0.9134,  1.2650, -0.4643,  1.8840,  0.7117,  2.4130,\n",
            "         0.2048, -1.0926,  0.4394,  1.8938, -1.0776,  0.9914, -0.9400, -2.3986,\n",
            "        -0.2427,  1.3008, -1.9200,  0.8954, -0.4401,  0.8827, -0.4798, -1.4758,\n",
            "         1.2836, -0.1805, -0.4816, -0.2760,  1.6470, -0.1118, -0.8365,  0.1684,\n",
            "        -1.7954, -0.9735,  0.5799,  0.4211, -1.4812,  1.1157, -0.2679, -0.6408,\n",
            "         1.2484, -0.5095, -1.9631,  0.5459,  0.1044, -1.2343,  0.5139,  0.5881,\n",
            "         0.1162, -0.2738, -0.5793,  0.1518, -0.4045,  1.3928,  0.3108,  1.4599,\n",
            "         1.3587, -2.6526, -0.5663, -0.1932, -0.8999, -0.5811, -0.4573, -1.7012,\n",
            "         1.2032,  1.1561,  1.1933, -0.2791, -0.9869, -0.3520, -0.6597, -0.7834,\n",
            "         0.7204, -1.2345, -1.2459,  0.1064,  0.0112,  0.9697, -0.8282,  0.4085,\n",
            "         1.1794,  2.0205, -0.7089, -1.9572,  0.5748,  0.9352,  1.7468,  0.5768,\n",
            "        -0.1249, -1.3372,  0.1723,  0.7698, -0.5745, -0.0663,  1.0823, -0.4991,\n",
            "         0.0141, -0.8050, -0.1266,  0.2844, -0.2807, -1.1418, -0.8157, -2.8404,\n",
            "        -1.3215, -0.8708, -0.7585, -0.1928,  1.8370, -1.1971,  1.6422,  0.1036,\n",
            "         0.4239, -0.0880, -0.4335,  0.2535,  0.3581,  1.3105, -1.2227,  0.9263,\n",
            "        -1.4019,  1.4063, -0.2750, -0.4298,  1.0046,  0.6141,  0.8264, -0.7495,\n",
            "        -0.7777,  1.1891,  0.7548, -0.3286, -0.9869,  0.0553, -0.2586, -0.6751,\n",
            "        -0.3019, -1.1858,  0.1773,  0.2152, -0.6795,  2.8313,  0.4556, -0.9063,\n",
            "         1.2855,  0.3523, -1.1810,  0.0307, -0.4763,  0.7259, -0.2709,  0.1562,\n",
            "         0.2727, -1.4994, -1.6063, -0.5182, -0.7206, -0.0982, -0.4226,  0.1816,\n",
            "         1.3604,  1.0642,  1.8175, -1.3229,  0.5442,  0.3059, -0.5853,  0.7586,\n",
            "         0.6729, -0.2004, -2.0274, -0.0760,  0.0795, -0.0951, -0.6511, -0.5367,\n",
            "        -0.4776,  0.6833,  0.2719, -1.3189,  1.1117,  1.0697,  0.4897, -0.6102,\n",
            "         0.2890,  0.5008,  0.9068,  1.0804,  0.1757, -0.6958, -2.3570, -1.5503,\n",
            "         0.1472, -0.2279,  0.4604,  0.1656,  0.5754, -1.8096,  1.4555, -0.0380,\n",
            "         0.1064, -1.2707,  0.1343, -2.8550,  1.3478, -0.7700,  0.2685,  0.4819,\n",
            "        -0.2677, -0.9938, -0.2680,  0.7606,  0.7689, -1.5875,  1.3593,  0.2062],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 가격\n",
            "tensor([-0.6129, -1.2798, -0.7241, -1.2136,  1.1379, -0.8829, -0.0355,  0.0048,\n",
            "        -1.6331, -0.3635,  0.8731,  0.3708, -0.0881,  1.7781,  0.5926,  0.0162,\n",
            "         0.5564, -0.4188,  1.2932,  0.7606,  0.4726, -0.9096, -1.0360,  0.2685,\n",
            "         1.2410,  1.9202,  0.5309, -0.1135,  0.5222, -0.9830, -0.1499,  0.5564,\n",
            "        -0.8832, -1.1202,  0.0975, -1.5120, -0.1394, -0.0917, -0.7951, -0.7590,\n",
            "        -1.1317, -1.7759,  0.4988,  0.2315, -0.8332, -0.5912, -1.1742,  0.6763,\n",
            "        -0.8777,  0.7218, -0.6251,  0.4564,  0.3788, -1.2035, -2.4246, -0.7911,\n",
            "        -0.9865, -1.9637,  0.8187,  1.8012,  0.9148,  1.5300,  1.5364,  0.8105,\n",
            "        -0.2349,  0.1876, -0.7077,  0.0772,  0.1555,  0.5943, -0.4711,  0.2236,\n",
            "        -0.2304, -1.0158, -2.4856,  0.3938,  1.1846, -0.0313,  0.2447,  0.6965,\n",
            "         1.4560,  0.5320,  1.2955, -1.5005,  0.4444,  0.3039,  1.5328,  2.3999,\n",
            "         0.3836, -0.1031,  0.0337,  0.3116,  1.2008,  0.2540, -0.6519,  1.4838,\n",
            "        -0.4675,  0.0301, -0.3766,  0.6824, -1.7291,  1.0771,  0.4388,  0.8275,\n",
            "        -0.5412,  0.0233,  1.3386, -0.3043, -0.3040, -0.3186,  0.3617,  1.1990,\n",
            "         1.7003,  0.4532,  0.5822,  1.1782,  0.0687,  0.7581, -0.5820, -0.4114,\n",
            "        -0.5215, -0.1603,  0.3081, -0.0625,  1.8329, -0.3754, -1.1448,  0.5914,\n",
            "         1.4281, -0.1960,  0.7828, -0.5762,  0.0453,  0.2761, -1.3134, -0.4120,\n",
            "         0.2007, -1.2381,  0.1374, -0.1749, -1.6024, -0.5424, -0.7587,  1.7367,\n",
            "        -0.1773, -0.1731,  2.8539,  0.1863, -0.5433, -0.8043, -1.1186, -0.2816,\n",
            "        -1.2694, -0.7565, -0.8225, -1.9958, -2.2046,  1.5920, -0.5284,  1.5173,\n",
            "        -1.3720,  0.6253, -0.1444,  0.8624, -0.6981,  1.3927, -0.7228,  0.1691,\n",
            "        -1.0107,  0.2947,  0.7992,  0.5066,  0.9753,  0.0790,  0.0138,  0.8783,\n",
            "        -1.9257,  0.7213, -1.1789,  0.6919,  0.2745,  1.0256,  0.6726,  0.9769,\n",
            "        -0.2063, -0.3660,  1.0315,  1.3042, -0.1884,  0.1953, -0.8911, -0.0159,\n",
            "         0.6911, -0.2865, -1.3480,  0.8759, -0.0486,  0.6850,  1.0806, -1.5466,\n",
            "        -0.4973, -1.8688,  0.0558, -2.2361,  0.2939,  0.9125,  0.8259,  0.4932,\n",
            "         1.9108,  0.5459,  0.4981,  1.0405,  0.2464,  0.6838,  1.7781, -0.1750,\n",
            "         0.3292,  0.1300, -1.1935, -0.5508, -1.7487, -0.5164,  2.3726,  0.3176,\n",
            "        -1.4814,  1.6783, -0.4452, -0.0999, -1.8786, -1.0073,  0.2982,  0.3398,\n",
            "        -1.3650, -0.9563,  0.4853, -0.6684,  0.4031, -1.1970,  0.4056, -1.0387,\n",
            "        -0.8148, -0.5009, -1.2679, -0.7138,  0.1258, -0.2558, -0.7547, -0.7427,\n",
            "        -0.6779,  0.1038, -0.6758, -0.5600,  0.4897,  1.5070, -0.4323,  0.2174],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l5cPRZZe-R4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71ca015-32dc-4091-e3e4-b014acc03bda"
      },
      "source": [
        "for word in test_words:\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "  emb = skipgram.embedding(input_id)\n",
        "\n",
        "  print(f\"Word: {word}\")\n",
        "  print(max(emb.squeeze(0))) # longtensor였던 임베딩벡터를 매트릭스형태로 변형하고 그중 가장 큰 원소값을 출력력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: 음식\n",
            "tensor(2.5706, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 맛\n",
            "tensor(2.6047, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 서비스\n",
            "tensor(2.9573, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 위생\n",
            "tensor(2.7450, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 가격\n",
            "tensor(3.7445, device='cuda:0', grad_fn=<UnbindBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4mv-fDF29Ha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "857e9afa-5cb5-4e2b-8870-7498659e935c"
      },
      "source": [
        "test_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['음식', '맛', '서비스', '위생', '가격']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jYY7xYd4vIR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d11349b6-df0d-4b60-c7bc-c44b566eef50"
      },
      "source": [
        "i2w[25]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'다시'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OBlu8O63CXx"
      },
      "source": [
        "def most_similar(word,top_k=5):\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "  input_emb = skipgram.embedding(input_id)\n",
        "  score=torch.matmul(input_emb,skipgram.embedding.weight.transpose(1,0)).view(-1)\n",
        "\n",
        "  _,top_k_ids=torch.topk(score,top_k)\n",
        "\n",
        "  return [i2w[word_id.item()] for word_id in top_k_ids][1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c75HQoLn2_Ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ef36ff-2619-4cbf-eeae-127b2a5adbe3"
      },
      "source": [
        "most_similar(\"맛\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['이', '방문', '기념일', '개선']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YvYEGNr2IiR"
      },
      "source": [
        "## Word2Vec 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcBVrabH2IiR"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8XdtYVf8Ydg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#matplotlib 패키지 한글 깨짐 처리 시작\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "#plt.rc('font', family='AppleGothic') #맥"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEypFpsw7K8q"
      },
      "source": [
        "pca=PCA(n_components=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NAllD3y7NFo"
      },
      "source": [
        "pc_weight=pca.fit_transform(skipgram.embedding.weight.data.cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KKgYYTa7Uh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5111c3b6-d104-4315-cddf-757cb9e058be"
      },
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "for word_id,(x_coordinate,y_coordinate) in enumerate(pc_weight):\n",
        "  plt.scatter(x_coordinate,y_coordinate,color=\"blue\")\n",
        "  plt.annotate(i2w[word_id], (x_coordinate, y_coordinate))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46020 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46020 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51060 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51060 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51339 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51339 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48324 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47196 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48324 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47196 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45320 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47924 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45320 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47924 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51020 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49885 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51020 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49885 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48708 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49828 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48708 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49828 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48169 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47928 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48169 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47928 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50948 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50948 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51328 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51328 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45908 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45908 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50640 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50640 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51312 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44552 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51312 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44552 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51221 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47568 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51221 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47568 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47579 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47579 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52628 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52380 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52628 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52380 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45824 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45824 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44163 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44163 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48372 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45800 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48372 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45800 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44032 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44201 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44032 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44201 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49912 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49912 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49910 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49910 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44033 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44033 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50504 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50504 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46300 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45348 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46300 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45348 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50836 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50836 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50756 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51204 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50756 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51204 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52572 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52572 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51116 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51116 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51032 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49324 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51032 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49324 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47564 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51313 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47101 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47564 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51313 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47101 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49345 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53468 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49345 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53468 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44060 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49440 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44060 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49440 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46104 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46104 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47476 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47476 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48148 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46972 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48148 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46972 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51649 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50896 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51649 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50896 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48516 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46308 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48516 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46308 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52828 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51208 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52828 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51208 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45392 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51068 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45392 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51068 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51004 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51004 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51676 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51676 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51200 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51200 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45716 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45716 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49888 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44221 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49888 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44221 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50024 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50024 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48520 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53132 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48520 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53132 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX4adG7B7qdV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}